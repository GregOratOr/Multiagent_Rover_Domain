{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f2a60bc-2920-4383-8d4c-106264cb3d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports go here\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# from parameters import parameters as p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38050443-a112-46a9-9564-4a72f7104716",
   "metadata": {},
   "source": [
    "## Parameters and Hyper Parameters\n",
    "\n",
    "<details>\n",
    "    <summary> Simulation Hyperparameters </summary>\n",
    "\n",
    "## Simulation Hyperparameters:\n",
    "1) Number_of_experiment : int\n",
    "2) Number_of_episodes : int\n",
    "3) Number_of_epochs : int\n",
    "4) POI_distribution : string\n",
    "5) Agent_distribution : string\n",
    "6) Reward_type : int\n",
    "</details>\n",
    "<details>\n",
    "    <summary> Domain Parameters </summary>\n",
    "\n",
    "## Domain Parameters:\n",
    "1) X_dimension : float\n",
    "2) y_dimension : float\n",
    "3) Number_of_POIs : int\n",
    "4) Number_of_agents : int\n",
    "</details>\n",
    "<details>\n",
    "    <summary> Fire Parameters </summary>\n",
    "\n",
    "## Fire Parameters:\n",
    "1) Value : float\n",
    "2) Level : int\n",
    "3) Coupling : int\n",
    "</details>\n",
    "<details>\n",
    "    <summary> Agent Parameters </summary>\n",
    "\n",
    "## Agent Parameters:\n",
    "1) Max_step : int\n",
    "2) Sensor_resolution : float\n",
    "3) Number_of_sectors : int\n",
    "4) Sensor_radius : float\n",
    "</details>\n",
    "<details>\n",
    "    <summary> Q-Learning Parameters </summary>\n",
    "\n",
    "## Q-Learning Parameters:\n",
    "1) Epsilon : float\n",
    "2) Epsilon_decay_factor : float\n",
    "3) Learning_rate : float\n",
    "4) Discount_factor : float\n",
    "</details>\n",
    "\n",
    "___\n",
    "<details>\n",
    "    <summary> Functions </summary>\n",
    "\n",
    "## Functions:\n",
    "1) \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c794de7b-dfa3-4645-b6c8-054f2ba90e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "# Domain Params\n",
    "parameters[\"x_dim\"] = 50.0\n",
    "parameters[\"y_dim\"] = 50.0\n",
    "parameters[\"n_pois\"] = 4\n",
    "parameters[\"n_agents\"] = 3\n",
    "\n",
    "# Fire Params\n",
    "parameters[\"value\"] = 100\n",
    "parameters[\"level\"] = int(2)\n",
    "parameters[\"hazard_coupling\"] =   {1: {\"coupling\": 1, \"opti_coupling\": 1},\n",
    "                                   2: {\"coupling\": 2, \"opti_coupling\": 2},\n",
    "                                   3: {\"coupling\": 3, \"opti_coupling\": 4},\n",
    "                                   4: {\"coupling\": 3, \"opti_coupling\": 5},\n",
    "                                  }\n",
    "parameters[\"coupling\"] = int(parameters[\"hazard_coupling\"].get(parameters[\"level\"], 1)[\"coupling\"])\n",
    "parameters[\"opti_coupling\"] = int(parameters[\"hazard_coupling\"].get(parameters[\"level\"], 1)[\"opti_coupling\"])\n",
    "\n",
    "# Agent Params\n",
    "parameters[\"max_step\"] = 1.5\n",
    "parameters[\"sensor_res\"] = 90.0 # Sector size in degrees\n",
    "parameters[\"n_sectors\"] = int(360.0 / parameters[\"sensor_res\"])\n",
    "parameters[\"sensor_radius\"] = 3.0\n",
    "parameters[\"observation_size\"] = int(2 * parameters[\"n_sectors\"])\n",
    "\n",
    "# Q-Learning Params\n",
    "parameters[\"epsilon\"] = 0.5\n",
    "parameters[\"epsilon_decay_factor\"] = 0.95\n",
    "parameters[\"learning_rate\"] = 0.1\n",
    "parameters[\"discount_factor\"] = 0.95\n",
    "\n",
    "# Simulation Hyperparams\n",
    "parameters[\"n_experiments\"] = 3\n",
    "parameters[\"n_episodes\"] = int(100)\n",
    "parameters[\"n_epochs\"] = int(1000)\n",
    "parameters[\"poi_distribution\"] = \"Random\"\n",
    "parameters[\"agent_distribution\"] = \"OneRandom\" # OneRandom, AllRandom\n",
    "parameters[\"reward_type\"] = 1 # 0: Global rewards, 1: Difference Rewards, 2: D++ Rewards\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8b5f84-90e5-42fa-a053-f8844fc05495",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Helper Functions\n",
    "\n",
    "<details>\n",
    "    <summary>Functions</summary>\n",
    "\n",
    "## Functions:\n",
    "1) save_poi_config_csv()\n",
    "2) save_agent_config_csv()\n",
    "3) get_angle()\n",
    "4) get_euclidean_distance()\n",
    "5) get_squared_distance()\n",
    "</details>\n",
    "\n",
    "## Description:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "442e5aa9-6c11-45ac-a7d6-798def627cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def save_poi_config_csv(pois_info, config_id):\n",
    "    \"\"\"\n",
    "    Saves POIs' configuration to a csv file in a folder called World_Config\n",
    "    \"\"\"\n",
    "    dir_name = './Configs'  # Intended directory for output files\n",
    "\n",
    "    if not os.path.exists(dir_name):  # If Data directory does not exist, create it\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "    pfile_name = os.path.join(dir_name, f'POI_Config{config_id}.csv')\n",
    "\n",
    "    with open(pfile_name, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for poi_id in range(len(pois_info)):\n",
    "            writer.writerow(pois_info[poi_id, :])\n",
    "\n",
    "    csvfile.close()\n",
    "\n",
    "def save_agent_config_csv(agent_infos, config_id):\n",
    "    \"\"\"\n",
    "    Saves Agents' configuration to a csv file in a folder called World_Config\n",
    "    \"\"\"\n",
    "    dir_name = './Configs'  # Intended directory for output files\n",
    "\n",
    "    if not os.path.exists(dir_name):  # If Data directory does not exist, create it\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "    pfile_name = os.path.join(dir_name, f'Agent_Config{config_id}.csv')\n",
    "\n",
    "    row = np.zeros(3)\n",
    "    with open(pfile_name, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for agent_id in range(len(agent_infos)):\n",
    "            writer.writerow(agent_infos[agent_id, :])\n",
    "\n",
    "    csvfile.close()\n",
    "\n",
    "def get_angle(source_x, source_y, target_x, target_y, radians=False):\n",
    "    radian_angle = math.atan2(target_y - source_y, target_x - source_x)\n",
    "    if radians:\n",
    "        return radian_angle\n",
    "    return math.degrees(radian_angle)\n",
    "\n",
    "def get_euclidean_distance(source_x, source_y, target_x, target_y):\n",
    "    d = math.sqrt((target_y - source_y)**2 + (target_x - source_x)**2)\n",
    "    if d < 0.01:\n",
    "        d = 0.01 \n",
    "    return d\n",
    "\n",
    "def get_squared_distance(source_x, source_y, target_x, target_y):\n",
    "    d =(target_y - source_y)**2 + (target_x - source_x)**2\n",
    "    if d < 0.0001:\n",
    "        d = 0.0001 \n",
    "    return d\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20956a91-4fe7-4ebd-8c43-51769bed2310",
   "metadata": {},
   "source": [
    "## Agent Class\n",
    "\n",
    "<details>\n",
    "    <summary> Parameters </summary>\n",
    "\n",
    "### Parameters:\n",
    "1) ID : int\n",
    "2) Location : (x: float, y: float, theta: float)\n",
    "4) Step_size : float\n",
    "5) Sensor_radius : float\n",
    "6) Sensor_resolution : float in degrees.\n",
    "7) Number_of_sectors : int (360/resolution)\n",
    "8) Current_observation : ndarray\n",
    "9) Past_observation : ndarray\n",
    "10) Action_space (up, down, left, right)\n",
    "11) Q-table : dict\n",
    "12) Local_reward : float\n",
    "13) Current_action_dir : float (angle phi)\n",
    "14) Learning_rate : float\n",
    "15) Discount_factor : float\n",
    "16) Epsilon : float \n",
    "17) Snapshot : list (stores parameters that remain same across configs)\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary> Functions </summary>\n",
    "    \n",
    "### Functions:\n",
    "1) reset()\n",
    "2) setup_sector_angles()\n",
    "3) get_sector_from_direction()\n",
    "4) get_direction_from_sector()\n",
    "5) move()\n",
    "6) turn()\n",
    "7) new_location()\n",
    "8) update_history()\n",
    "9) get_Qvalue()\n",
    "10) set_Qvalue()\n",
    "11) update_Qvalue()\n",
    "12) scan_surrounding()\n",
    "13) scan_for_pois()\n",
    "14) scan_for_agents()\n",
    "15) **select_action()**\n",
    "16) e_greedy()\n",
    "</details>\n",
    "\n",
    "### Desciption:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e8cad18-0ba0-4fa9-9f39-24fab9d786cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, agent_id, x_pos, y_pos, theta):\n",
    "        self.state_size = 8 # (size of the encoding for observations from environment) This needs to change when the actual observation size is known.\n",
    "        self.agent_id = agent_id\n",
    "        self.loc = np.array([x_pos, y_pos, theta])\n",
    "        self.max_step = p[\"max_step\"]\n",
    "        \n",
    "        # Sensor specs.\n",
    "        self.sensor_range = p[\"sensor_radius\"]\n",
    "        self.sensor_res = p[\"sensor_res\"]\n",
    "        self.n_sectors = p[\"n_sectors\"]\n",
    "        self.sector_angles = self.setup_sector_angles()\n",
    "        \n",
    "        # MDP params\n",
    "        self.curr_obs = np.zeros(self.state_size)\n",
    "        self.prev_obs = np.zeros(self.state_size)\n",
    "        self.action_space = [i for i in range(self.n_sectors)] # 0 - Front, 1 - Left, 2 - Back, 3 - Right\n",
    "        self.Qtable = {}\n",
    "        self.l_reward = 0.0\n",
    "        self.curr_action_dir = 0.0\n",
    "        self.lr = p[\"learning_rate\"]\n",
    "        self.dscf = p[\"discount_factor\"]\n",
    "\n",
    "        # Logging info\n",
    "        self.snap = [self.lr, self.dscf]\n",
    "        self.path = [self.loc.copy()]\n",
    "\n",
    "    def reset(self, config):\n",
    "        self.loc[0] = config[0]\n",
    "        self.loc[1] = config[1]\n",
    "        self.loc[2] = config[2]\n",
    "        self.curr_obs = np.zeros(self.state_size)\n",
    "        self.prev_obs = self.curr_obs.copy()\n",
    "        self.l_reward = 0.0\n",
    "        self.curr_action_dir = 0.0\n",
    "        self.lr = self.snap[0]\n",
    "        self.dscf = self.snap[1]\n",
    "        self.path = [self.loc.copy()]\n",
    "\n",
    "    def setup_sector_angles(self):\n",
    "        half = self.sensor_res/2\n",
    "        angles = []\n",
    "        n = self.n_sectors\n",
    "        \n",
    "        s = 90.0 - half\n",
    "        for i in range(n):\n",
    "            angles.append(s)\n",
    "            s += self.sensor_res\n",
    "        return angles\n",
    "        \n",
    "    def get_sector_from_direction(self, direction):\n",
    "        # direction should be in range [0, 360)\n",
    "        # direction = direction - self.loc[2]  # This line will change direction from world coordinate system to local coordiate system.\n",
    "        if direction < 0:\n",
    "            while direction < 0:\n",
    "                direction += 360\n",
    "        elif direction >= 360:\n",
    "            while direction >= 360:\n",
    "                direction -= 360\n",
    "\n",
    "        for i in range(len(self.sector_angles) - 1):\n",
    "            if direction >= self.sector_angles[i] and direction < self.sector_angles[i + 1]:\n",
    "                # print(\"Quadrant = \", i)\n",
    "                return i, direction\n",
    "                \n",
    "        # print(\"Quadrant = \", len(self.sector_angles) - 1)\n",
    "        return len(self.sector_angles) - 1, direction\n",
    "\n",
    "    def get_direction_from_sector(self, sector):\n",
    "        direction = self.sector_angles[sector] + self.sensor_res/2\n",
    "        if direction >= 360.0:\n",
    "            direction -= 360.0\n",
    "        rad_angle = math.radians(direction) \n",
    "        dir_vector = [math.cos(rad_angle), math.sin(rad_angle)]\n",
    "        return direction, dir_vector\n",
    "\n",
    "    def move(self, new_location):\n",
    "        self.loc[:2] = new_location\n",
    "        self.update_history()\n",
    "\n",
    "    def turn(self, new_direction):\n",
    "        self.loc[2] = new_direction\n",
    "        self.update_history()\n",
    "\n",
    "    def new_location(self, new_location):\n",
    "        self.loc = new_location\n",
    "        self.update_history()\n",
    "    \n",
    "    def update_history(self):\n",
    "        self.path.append(self.loc.copy())\n",
    "\n",
    "    def get_Qvalue(self, observation, action):\n",
    "        return self.Qtable.get((tuple(observation), action), 0.0)\n",
    "\n",
    "    def set_Qvalue(self, observation, action, value):\n",
    "        self.Qtable[(tuple(observation), action)] = value\n",
    "    \n",
    "    def update_Qvalue(self):\n",
    "        curr_Q = self.get_Qvalue(self.curr_obs.tolist(), self.get_sector_from_direction(self.curr_action_dir))\n",
    "        next_Qs = [self.get_Qvalue(self.curr_obs.tolist(), next_sector) for next_sector in range(self.n_sectors)]\n",
    "        best_Q = np.max(next_Qs)\n",
    "        td_error = self.l_reward + self.dscf * (best_Q - curr_Q)\n",
    "        new_Q = curr_Q + self.lr * td_error\n",
    "        self.set_Qvalue(self.prev_obs.tolist(), self.get_sector_from_direction(self.curr_action_dir), new_Q)\n",
    "\n",
    "    def scan_surrounding(self, agents, pois, distance_table):\n",
    "        observation_for_pois = self.scan_for_pois(pois, distance_table)\n",
    "        observation_for_other_agents = self.scan_for_agents(agents)\n",
    "        self.prev_obs = self.curr_obs.copy()\n",
    "        self.curr_obs = np.concatenate((observation_for_pois, observation_for_other_agents))\n",
    "    \n",
    "    def scan_for_pois(self, pois, dist_table):\n",
    "        poi_state = np.zeros(self.n_sectors)\n",
    "        state_mask = np.zeros(self.n_sectors)\n",
    "        dist_sq_to_pois = dist_table[:, self.agent_id]\n",
    "        \n",
    "        detection_radius_sq = self.sensor_range**2\n",
    "        poi_ids_in_range = np.where(dist_sq_to_pois <= detection_radius_sq)[0]\n",
    "        # poi_ids_out_range = np.where(dist_sq_to_pois > detection_radius_sq)[0]\n",
    "        \n",
    "        for poi_id in poi_ids_in_range:\n",
    "            poi = pois[poi_id]\n",
    "            angle = get_angle(self.loc[0], self.loc[1], poi.loc[0], poi.loc[1])\n",
    "            poi_sector, angle = self.get_sector_from_direction(angle)\n",
    "            dist_sq = dist_sq_to_pois[poi_id]\n",
    "            if poi.poi_id in poi_ids_in_range:\n",
    "                # Detected the POI\n",
    "                state_mask[poi_sector] = 1\n",
    "                poi_state[poi_sector] += float(poi.val/dist_sq) \n",
    "\n",
    "        for idx in range(self.n_sectors):\n",
    "            if state_mask[idx] == 0:\n",
    "                poi_state[idx] = -1\n",
    "        return poi_state\n",
    "    \n",
    "    def scan_for_agents(self, agents):\n",
    "        agent_state = np.zeros(self.n_sectors)\n",
    "        state_mask = np.zeros(self.n_sectors)\n",
    "\n",
    "        x_self, y_self, theta_self = self.loc\n",
    "        detection_radius_sq = self.sensor_range**2\n",
    "\n",
    "        for other_agent in agents:\n",
    "            if other_agent.agent_id == self.agent_id:\n",
    "                continue\n",
    "\n",
    "            x_other, y_other, theta_other = other_agent.loc\n",
    "            dist_sq = (x_other - x_self)**2 + (y_other - y_self)**2\n",
    "            if dist_sq < 0.01:\n",
    "                dist_sq = 0.01\n",
    "            if dist_sq <= detection_radius_sq:\n",
    "                angle = get_angle(x_self, y_self, x_other, y_other)\n",
    "                agent_sector, angle = self.get_sector_from_direction(angle)\n",
    "\n",
    "                state_mask[agent_sector] = 1\n",
    "                agent_state[agent_sector] += 1/dist_sq\n",
    "        for idx in range(self.n_sectors):\n",
    "            if state_mask[idx] == 0:\n",
    "                agent_state[idx] = -1\n",
    "        return agent_state\n",
    "    \n",
    "    def e_greedy(self, epsilon):\n",
    "        if np.random.rand() < epsilon:\n",
    "            # We explore\n",
    "            angle = np.random.uniform(0.0, 360.0)\n",
    "            rad_angle = math.radians(angle)\n",
    "            dir_vector = [math.cos(rad_angle), math.sin(rad_angle)]\n",
    "            return dir_vector, angle\n",
    "        else:\n",
    "            # We exploit\n",
    "            # q_values = []\n",
    "            max_Qvalue = -1000.0\n",
    "            best_action_sector = None\n",
    "            for action in range(self.n_sectors): # Action here is a sector of choice NOT a direction vector.\n",
    "                # q_values.append(self.get_Qvalue(self.curr_obs.tolist(), action))\n",
    "                Q_val = self.get_Qvalue(self.curr_obs.tolist(), action)\n",
    "                if Q_val > max_Qvalue:\n",
    "                    # found new max value\n",
    "                    best_action_sector = action\n",
    "                    max_Qvalue = Q_val\n",
    "            # max_Qvalue = np.max(q_values)\n",
    "            # best_action_sector = np.argmax(q_values)\n",
    "            \n",
    "            angle, dir_vector = self.get_direction_from_sector(best_action_sector)\n",
    "            return dir_vector, angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12e87bf4-e8cf-41ba-9166-1b595464d624",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def get_sector_from_direction(res, n_sect, direction):\n",
    "#     # direction should be in range [0, 360)\n",
    "#     if direction < 0:\n",
    "#         while direction < 0:\n",
    "#             direction += 360\n",
    "#     elif direction >= 360:\n",
    "#         while direction >= 360:\n",
    "#             direction -= 360\n",
    "\n",
    "#     half = res/2\n",
    "#     angles = []\n",
    "#     n = n_sect\n",
    "        \n",
    "#     s = 90.0 - half\n",
    "    \n",
    "#     for i in range(n):\n",
    "#         angles.append(s)\n",
    "#         s += res\n",
    "        \n",
    "#     flag = False\n",
    "#     for i in range(len(angles) - 1):\n",
    "#         if direction >= angles[i] and direction < angles[i + 1]:\n",
    "#             flag = True\n",
    "#             print(\"Quadrant = \", i, direction)\n",
    "#     if not flag:        \n",
    "#         if (direction >= 0.0 and direction < angles[0]) or (direction >= angles[-1] and direction < 360): \n",
    "#             print(\"Quadrant = \", len(angles) - 1, direction)\n",
    "#     print(angles)\n",
    "#     return angles\n",
    "\n",
    "\n",
    "# def get_direction_from_sector(angles, res, sector):\n",
    "#     sector_angles = angles\n",
    "#     direction = sector_angles[sector] + res/2\n",
    "    \n",
    "#     # if sector >= len(sector_angles) - 1:\n",
    "#     if direction >= 360.0:\n",
    "#         direction -= 360.0\n",
    "#     print(direction)\n",
    "    \n",
    "# res = 60\n",
    "# n_sect = int(360/res)\n",
    "# sect_angles = get_sector_from_direction(res, n_sect, 40)\n",
    "# for i in range(n_sect):\n",
    "#     get_direction_from_sector(sect_angles, res, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31498530-1492-4638-bf1d-7970cafba20f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Point of Interest Class\n",
    "\n",
    "<details>\n",
    "    <summary> Parameters </summary>\n",
    "\n",
    "### Parameters:\n",
    "1) ID : int\n",
    "2) Location : (x: float, y: float)\n",
    "3) Value : float\n",
    "4) Level : int (what is the hazard level of POI)\n",
    "5) Coupling : int (minimum number of agents reqired)\n",
    "6) Detected : bool (True if it's detected, False otherwise)\n",
    "7) Done : bool (True if it's harvested, False otherwise)\n",
    "8) Snapshot : list (stores parameters that stay constant across configs)\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary> Functions </summary>\n",
    "    \n",
    "### Functions:\n",
    "1) reset()\n",
    "2) set_detected()\n",
    "3) set_done()\n",
    "</details>\n",
    "\n",
    "### Desciption:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a5e4f61-824e-49ae-bf24-e8fb3dd1bcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POI:\n",
    "    def __init__(self, poi_id, x_pos, y_pos, value, level, coupling, opti_coupling):\n",
    "        self.poi_id = int(poi_id)\n",
    "        self.loc = np.array([x_pos, y_pos])\n",
    "        self.val = value\n",
    "        self.level = int(level)\n",
    "        self.coupling = int(coupling)\n",
    "        self.opti_coupling = int(opti_coupling)\n",
    "        self.detected = False\n",
    "        self.done = False\n",
    "        self.snap = [self.detected]\n",
    "\n",
    "    def reset(self, config):\n",
    "        self.loc[0] = config[0]\n",
    "        self.loc[1] = config[1]\n",
    "        self.val = config[2]\n",
    "        self.level = config[3]\n",
    "        self.coupling = config[4]\n",
    "        self.opti_coupling = config[5]\n",
    "        self.detected = self.snap[0]\n",
    "        self.done = False\n",
    "\n",
    "    def set_detected(self, flag):\n",
    "        self.detected = flag\n",
    "\n",
    "    def set_done(self, flag):\n",
    "        self.done = flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa1266f-aa90-42ef-9ce2-f4ec87cdef6f",
   "metadata": {},
   "source": [
    "## Forest Domain Class\n",
    "\n",
    "<details>\n",
    "    <summary> Parameters </summary>\n",
    "\n",
    "### Parameters:\n",
    "1) X : float\n",
    "2) Y : float\n",
    "3) Number of POI : int\n",
    "4) Number of Agents : int\n",
    "5) Done : bool (True if all fires are done, False otherwise)\n",
    "6) Global_rewards : list (one reward for each poi)\n",
    "7) Agents : list (objects of Agent Class)\n",
    "8) Agent_configurations : 2D List (stores different positional configurations of Agents) \n",
    "9) POIs : list (objects of POI Class)\n",
    "10) POI_configurations : 2D List (stores different positional configurations of POIs)\n",
    "11) POI_agent_distances : 2D array of floats (distance squared values for each poi and agent pair)\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary> Functions </summary>\n",
    "    \n",
    "### Functions:\n",
    "1) reset()\n",
    "2) **show_simulation_details()**\n",
    "3) goals_done()\n",
    "4) create_forest()\n",
    "5) setup_forest()\n",
    "6) create_poi_config()\n",
    "7) create_agent_config()\n",
    "6) load_poi_config()\n",
    "7) load_agent_config()\n",
    "8) update_distance_table()\n",
    "9) select_joint_action()\n",
    "10) calculate_global_reward()\n",
    "11) **calculate_difference_reward()**\n",
    "12) step()\n",
    "13) execute()\n",
    "</details>\n",
    "\n",
    "### Desciption:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87a21830-d895-488c-831f-c3bee6cdca05",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ForestDomain:\n",
    "    def __init__(self):\n",
    "        self.X = p[\"x_dim\"]\n",
    "        self.Y = p[\"y_dim\"]\n",
    "        self.n_pois = p[\"n_pois\"]\n",
    "        self.n_agents = p[\"n_agents\"]\n",
    "        self.done = False\n",
    "        self.global_rewards = np.zeros(self.n_pois)\n",
    "        self.agents = np.empty(self.n_agents, dtype=object)\n",
    "        self.agent_configs = [[] for _ in range(self.n_agents)]\n",
    "        self.pois = np.empty(self.n_pois, dtype=object)\n",
    "        self.poi_configs = [[] for _ in range(self.n_pois)]\n",
    "        self.distance_table = np.ones((self.n_pois, self.n_agents)) # stores squared eucliedian distances\n",
    "        \n",
    "    def reset(self, config_id):\n",
    "        self.done = False\n",
    "        self.global_rewards = np.zeros(self.n_pois)\n",
    "        self.distance_table[:, :] = 1000.0\n",
    "        for idx, poi in enumerate(self.pois):\n",
    "            poi.reset(self.poi_configs[idx][config_id])\n",
    "        for idx, agent in enumerate(self.agents):\n",
    "            agent.reset(self.agent_configs[idx][config_id])\n",
    "        \n",
    "    def goals_done(self):\n",
    "        for poi in self.pois:\n",
    "            if poi.done:\n",
    "                continue\n",
    "            else:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def create_forest(self, config_id):\n",
    "        poi_infos = self.create_poi_config()\n",
    "        save_poi_config_csv(poi_infos, config_id)\n",
    "        \n",
    "        agent_infos = self.create_agent_config(poi_infos)\n",
    "        save_agent_config_csv(agent_infos, config_id)\n",
    "        \n",
    "    def setup_forest(self):\n",
    "        f1 = self.load_poi_config()\n",
    "        # print(self.pois[:].poi_id)\n",
    "        f2 = self.load_agent_config()\n",
    "        if f1 and f2:\n",
    "            print(\"Forest Ready.\")\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def create_poi_config(self):\n",
    "        all_poi_params = np.zeros((self.n_pois, 7)) # (n_pois) X (poi_id, x_pos, y_pos, value, level, coupling, opti_coupling)\n",
    "        if p[\"poi_distribution\"] == \"Random\":\n",
    "            for idx in range(self.n_pois):\n",
    "                # all_poi_params[idx, 0] = int(idx)\n",
    "                \n",
    "                x = random.uniform(0, self.X - 1.0)\n",
    "                y = random.uniform(0, self.Y - 1.0)\n",
    "    \n",
    "                too_close = True\n",
    "                while too_close:\n",
    "                    count = 0\n",
    "                    for i in range(self.n_pois):\n",
    "                        if i != idx:\n",
    "                            x_dist = x - all_poi_params[idx, 1]\n",
    "                            y_dist = y - all_poi_params[idx, 2]\n",
    "                            dist_sq = x_dist** 2 + y_dist**2\n",
    "                            min_dist_sq = (p[\"sensor_radius\"] * 2.1)** 2\n",
    "                            if dist_sq <= min_dist_sq:\n",
    "                                count += 1\n",
    "                    if count == 0:\n",
    "                        too_close = False\n",
    "                    else:\n",
    "                        x = random.uniform(0, self.X - 1.0)\n",
    "                        y = random.uniform(0, self.Y - 1.0)\n",
    "                all_poi_params[idx, 0] = x\n",
    "                all_poi_params[idx, 1] = y\n",
    "                all_poi_params[idx, 2] = p[\"value\"]\n",
    "                all_poi_params[idx, 3] = p[\"level\"]\n",
    "                all_poi_params[idx, 4] = p[\"coupling\"]\n",
    "                all_poi_params[idx, 5] = p[\"opti_coupling\"]\n",
    "        print(\"POI Configurations generated.\")\n",
    "        return all_poi_params\n",
    "\n",
    "    def create_agent_config(self, poi_info):\n",
    "        all_agent_params = np.zeros((self.n_agents, 4)) # (n_agents) X (agent_id, x_pos, y_pos, theta)\n",
    "        if p[\"agent_distribution\"] == \"OneRandom\":\n",
    "            x_pos = random.uniform(0.0, self.X - 1.0)\n",
    "            y_pos = random.uniform(0.0, self.Y - 1.0)\n",
    "            theta = random.uniform(0.0, 360.0)\n",
    "            buffer = 3.0\n",
    "            \n",
    "            too_close = True\n",
    "            while too_close:\n",
    "                count = 0\n",
    "                for poi_idx in range(self.n_pois):\n",
    "                    x_dist = x_pos - poi_info[poi_idx, 1]\n",
    "                    y_dist = y_pos - poi_info[poi_idx, 2]\n",
    "                    dist_sq = x_dist**2 + y_dist**2\n",
    "                    min_dist_sq = (p[\"sensor_radius\"] * 2.0 + buffer)** 2\n",
    "                    if dist_sq <= min_dist_sq:\n",
    "                        count += 1\n",
    "                \n",
    "                if count == 0:\n",
    "                    too_close = False\n",
    "                else:\n",
    "                    x_pos = random.uniform(0.0, self.X - 1.0)\n",
    "                    y_pos = random.uniform(0.0, self.Y - 1.0)\n",
    "            \n",
    "            for idx in range(self.n_agents):\n",
    "                # all_agent_params[idx, 0] = int(idx)\n",
    "                all_agent_params[idx, 0] = x_pos\n",
    "                all_agent_params[idx, 1] = y_pos\n",
    "                all_agent_params[idx, 2] = theta\n",
    "                \n",
    "        # elif p[\"agent_distribution\"] == \"AllRandom\":\n",
    "        print(\"Agent Configurations generated.\")\n",
    "        return all_agent_params\n",
    "    \n",
    "    def load_poi_config(self):\n",
    "        for cf_id in range(p[\"n_experiments\"]):\n",
    "            csv_input = []\n",
    "            status = False\n",
    "            with open(f'./Configs/POI_Config{cf_id}.csv', mode='r') as csvfile:\n",
    "                csv_reader = csv.reader(csvfile, delimiter=',')\n",
    "                for row in csv_reader:\n",
    "                    csv_input.append(row)\n",
    "    \n",
    "            for idx in range(self.n_pois):\n",
    "                poi_id = idx #int(float(csv_input[idx][0]))\n",
    "                x_pos = float(csv_input[idx][0])\n",
    "                y_pos = float(csv_input[idx][1])\n",
    "                value = float(csv_input[idx][2])\n",
    "                level = int(float(csv_input[idx][3]))\n",
    "                coupling = int(float(csv_input[idx][4]))\n",
    "                opti_coupling = int(float(csv_input[idx][5]))\n",
    "                \n",
    "                if cf_id == 0:\n",
    "                    self.pois[idx] = POI(poi_id, x_pos, y_pos, value, level, coupling, opti_coupling)\n",
    "                    \n",
    "                self.poi_configs[poi_id].append((x_pos, y_pos, value, level, coupling, opti_coupling))\n",
    "\n",
    "        status = True\n",
    "        return status\n",
    "\n",
    "    def load_agent_config(self):\n",
    "        status = False\n",
    "        for cf_id in range(p[\"n_experiments\"]):\n",
    "            csv_input = []\n",
    "            with open(f'./Configs/Agent_Config{cf_id}.csv', mode='r') as csvfile:\n",
    "                csv_reader = csv.reader(csvfile, delimiter=',')\n",
    "\n",
    "                for row in csv_reader:\n",
    "                    csv_input.append(row)\n",
    "\n",
    "            for idx in range(self.n_agents):\n",
    "                agent_id = idx #int(float(csv_input[idx][0]))\n",
    "                x_pos = float(csv_input[idx][0])\n",
    "                y_pos = float(csv_input[idx][1])\n",
    "                theta = float(csv_input[idx][2])\n",
    "\n",
    "                if cf_id == 0:\n",
    "                    self.agents[idx] = Agent(agent_id, x_pos, y_pos, theta)\n",
    "\n",
    "                self.agent_configs[idx].append((x_pos, y_pos, theta))\n",
    "                \n",
    "        status = True\n",
    "        return status\n",
    "\n",
    "    def update_distance_table(self):\n",
    "        for poi in self.pois:\n",
    "            # for each poi\n",
    "            poi_x, poi_y = poi.loc\n",
    "            for agent in self.agents:\n",
    "                a_x, a_y, a_t = agent.loc\n",
    "                dist_sq = (poi_x - a_x)**2 + (poi_y - a_y)**2\n",
    "                if dist_sq < 0.01:\n",
    "                    dist_sq = 0.01\n",
    "                self.distance_table[poi.poi_id, agent.agent_id] = dist_sq\n",
    "        '''\n",
    "        print(\"ForestDomain:: Distance Table: \\n\")\n",
    "        for r in self.distance_table:\n",
    "            for c in r:\n",
    "                print(c, \" \")\n",
    "            print(\"\\n\")\n",
    "        '''\n",
    "    \n",
    "    def select_joint_action(self, epsilon):\n",
    "        joint_action = np.zeros((self.n_agents, 2)) # returns x, y direction vector for action taken.\n",
    "        for agent in self.agents:\n",
    "            action, angle = agent.e_greedy(epsilon)\n",
    "            joint_action[agent.agent_id] = action\n",
    "        return joint_action\n",
    "    \n",
    "    def calc_global_reward(self):\n",
    "        global_reward = np.zeros(self.n_pois)\n",
    "        # Calculate the global rewards for each POI.\n",
    "        for poi in self.pois:\n",
    "            observers = 0\n",
    "            # Check which agents have detected the POI.\n",
    "            detection_radius_sq = p[\"sensor_radius\"]**2\n",
    "            observer_agent_ids = np.where(self.distance_table[poi.poi_id] <= detection_radius_sq)[0]\n",
    "            observers = len(observer_agent_ids)\n",
    "            if observers > 1:\n",
    "                poi.set_detected(True)\n",
    "            # Get just observer count\n",
    "            '''\n",
    "            sorted_dist_sq_from_poi = np.sort(self.distance_table[poi.poi_id])\n",
    "            for i in range(poi.coupling):\n",
    "                if sorted_dist_sq_from_poi[i] < detection_radius_sq:\n",
    "                    observers += 1\n",
    "             '''       \n",
    "            if observers >= int(poi.coupling):\n",
    "                # We have minimum required agents\n",
    "                dist_sq_sum = np.sum(np.partition(self.distance_table[poi.poi_id], observers-1)[:observers])\n",
    "                v = poi.val/dist_sq_sum\n",
    "                reward_full = v  # Add any other partial rewards gained in the process here.\n",
    "                optimum_coupling = poi.opti_coupling  # When there is a different optimum value than the minimum, change this. \n",
    "                global_reward[poi.poi_id] = reward_full * np.exp(-1 * float(observers/optimum_coupling)) \n",
    "                flag = True\n",
    "                # If the agent needs to be on the POI's location to complete it. Un-comment the block below. \n",
    "                ''' \n",
    "                observers_on_poi = 0\n",
    "                for i in range(observers_in_range):\n",
    "                    if sorted_dist_sq_from_poi[i] <= 0.001:\n",
    "                        # Has reached poi\n",
    "                        continue\n",
    "                    else:\n",
    "                        # Atleast 1 is missing.\n",
    "                        flag = False\n",
    "                '''\n",
    "                poi.set_done(flag)\n",
    "        self.global_rewards = global_reward\n",
    "\n",
    "    def calc_difference_reward(self):\n",
    "        difference_rewards = np.zeros(self.n_agents)\n",
    "        for agent in self.agents:\n",
    "            counterfactual = 0.0\n",
    "            counterfactual_rewards = np.zeros(self.n_pois)\n",
    "            detection_radius_sq = agent.sensor_range**2\n",
    "            for poi in self.pois:\n",
    "                observers = 0\n",
    "                distances = self.distance_table[poi.poi_id, :]\n",
    "                agents_in_radius = distances <= detection_radius_sq\n",
    "                agents_in_radius[agent.agent_id] = False\n",
    "\n",
    "                observers = np.sum(agents_in_radius)\n",
    "                if observers >= poi.coupling:\n",
    "                    dist_sq_sum = np.sum(distances[agents_in_radius])\n",
    "                    v = poi.val/dist_sq_sum\n",
    "                    counterfactual_rewards[poi.poi_id] = v * np.exp(-1 * float(observers/poi.opti_coupling))\n",
    "            difference_rewards[agent.agent_id] = np.sum(self.global_rewards - counterfactual_rewards)\n",
    "        return difference_rewards  \n",
    "\n",
    "    def calc_difference_plus_plus(self):\n",
    "        difference_rewards = self.calc_difference_reward()\n",
    "        dpp_rewards = np.zeros(self.n_agents)\n",
    "        n_ghost_agents = self.n_agents - 1\n",
    "        n_ghost_opti = 0\n",
    "        new_n_agents = self.n_agents + n_ghost\n",
    "        for agent in self.agents:\n",
    "            detection_radius_sq = agent.sensor_range**2\n",
    "            ghost_global = np.zeros(self.n_pois)\n",
    "            \n",
    "            n_ghost = 1\n",
    "            while n_ghost <= n_ghost_agents:\n",
    "                poi_rewards = np.zeros(self.n_pois)\n",
    "                for poi in self.pois:\n",
    "                    observers = 0\n",
    "                    dist_sq = self.distance_table[poi.poi_id]\n",
    "                    ghost_dist_to_add = np.full(n_ghost, dist_sq[agent.agent_id])\n",
    "                    dist_sq_ghost = np.append(dist_sq, dist_sq[agent.agent_id])\n",
    "                    agents_in_radius = dist_sq_ghost <= detection_radius_sq\n",
    "                    observers = np.sum(agents_in_radius)\n",
    "    \n",
    "                    if observers >= poi.coupling:\n",
    "                        dist_sq_sum = np.sum(dist_sq_ghost[agents_in_radius])\n",
    "                        v = poi.val/dist_sq_sum\n",
    "                        poi_rewards[poi.poi_id] = v * np.exp(-1 * float(observers/poi.opti_coupling))\n",
    "                if np.sum(poi_rewards) > np.sum(ghost_global):\n",
    "                    ghost_global = poi_rewards.copy()\n",
    "                    n_ghost_opti = n_ghost\n",
    "                    n_ghost += 1\n",
    "                else:\n",
    "                    # Found the optimal in previous iteration.\n",
    "                    break\n",
    "            # ghost_global has the global value with n_ghost agents added.\n",
    "            dpp_rewards[agent.agent_id] = np.sum(ghost_global - self.global_rewards)/(n_ghost_opti + 1.0)\n",
    "            \n",
    "        return np.maximum(dpp_rewards, difference_rewards)\n",
    "\n",
    "    def step(self, joint_action):\n",
    "        # Take the joint_action\n",
    "        for agent in self.agents:\n",
    "            # Calculate displacement.\n",
    "            dx = 2 * agent.max_step * (joint_action[agent.agent_id, 0] - 0.5)\n",
    "            dy = 2 * agent.max_step * (joint_action[agent.agent_id, 1] - 0.5)\n",
    "\n",
    "            # Get new coordinates in world.\n",
    "            x = np.clip(agent.loc[0] + dx, 0, self.X - 1)\n",
    "            y = np.clip(agent.loc[1] + dy, 0, self.Y - 1)\n",
    "\n",
    "            # Move the agent to new position.\n",
    "            agent.move((x, y))\n",
    "\n",
    "        # Update Distance table.\n",
    "        self.update_distance_table()\n",
    "        \n",
    "        # Fetch new observations.\n",
    "        for agent in self.agents:\n",
    "            agent.scan_surrounding(self.agents, self.pois, self.distance_table)\n",
    "\n",
    "        # Calculate global reswards for the system after taking this step.\n",
    "        self.calc_global_reward()\n",
    "\n",
    "    def execute(self, epsilon, step,  reward_type=0):\n",
    "        joint_action = self.select_joint_action(epsilon)\n",
    "\n",
    "        self.step(joint_action)\n",
    "        rewards = np.zeros(self.n_agents)\n",
    "        match reward_type:\n",
    "            case 0:\n",
    "                # Global rewards\n",
    "                reward = np.full(self.n_agents, np.sum(self.global_rewards))\n",
    "            case 1:\n",
    "                # Difference rewards\n",
    "                reward = self.calc_difference_reward()\n",
    "                # print(\"Using Difference Rewards.\")\n",
    "            case 2:\n",
    "                # D++ rewards\n",
    "                reward = self.difference_plus_plus()\n",
    "                # print(\"Using D++ Rewards.\")\n",
    "            case _:\n",
    "                print(\"Incorrect Reward code.\")\n",
    "                \n",
    "        for agent in self.agents:\n",
    "            agent.l_reward = reward[agent.agent_id]\n",
    "            agent.update_Qvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c250d5ab-83b7-4d85-b5b1-8a6db2db8f37",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def load_poi_config(n_pois, X, Y, radius, value, level, coupling):\n",
    "#     all_poi_params = np.zeros((n_pois, 6)) # (n_pois) X (poi_id, x_pos, y_pos, value, level, coupling)\n",
    "#     pois = np.empty(n_pois, dtype=object)\n",
    "#     for idx in range(n_pois):\n",
    "        # all_poi_params[idx, 0] = idx\n",
    "#         x = random.uniform(0, X - 1.0)\n",
    "#         y = random.uniform(0, Y - 1.0)\n",
    "\n",
    "#         too_close = True\n",
    "#         while too_close:\n",
    "#             count = 0\n",
    "#             for i in range(n_pois):\n",
    "#                 if i != idx:\n",
    "#                     x_dist = x - all_poi_params[idx, 1]\n",
    "#                     y_dist = y - all_poi_params[idx, 2]\n",
    "#                     dist_sq = x_dist** 2 + y_dist**2\n",
    "#                     min_dist_sq = (radius * 2.1)** 2\n",
    "#                     if dist_sq < min_dist_sq:\n",
    "#                         count += 1\n",
    "#             if count == 0:\n",
    "#                 too_close = False\n",
    "#             else:\n",
    "#                 x = random.uniform(0, X - 1.0)\n",
    "#                 y = random.uniform(0, Y - 1.0)\n",
    "#         all_poi_params[idx, 1] = x\n",
    "#         all_poi_params[idx, 2] = y\n",
    "#         all_poi_params[idx, 3] = value\n",
    "#         all_poi_params[idx, 4] = level\n",
    "#         all_poi_params[idx, 5] = coupling\n",
    "#         pois[idx] = POI(*all_poi_params[idx])\n",
    "#     for poi in pois:\n",
    "#         print(poi.poi_id, poi.loc, poi.val)\n",
    "\n",
    "# load_poi_config(2, 100.0, 100.0, 3, 100, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a7c61c5-a096-4e7d-a3f8-7130da2d5527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POI Configurations generated.\n",
      "Agent Configurations generated.\n",
      "POI Configurations generated.\n",
      "Agent Configurations generated.\n",
      "POI Configurations generated.\n",
      "Agent Configurations generated.\n",
      "Forest Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current Episode: 100%|██████████| 100/100 [00:14<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1/3 completed.\n",
      "Forest Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current Episode: 100%|██████████| 100/100 [00:14<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 2/3 completed.\n",
      "Forest Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current Episode: 100%|██████████| 100/100 [00:14<00:00,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 3/3 completed.\n",
      "            0\n",
      "0    0.000000\n",
      "1    0.000000\n",
      "2    3.253308\n",
      "3    0.000000\n",
      "4    0.000000\n",
      "..        ...\n",
      "95   0.000000\n",
      "96  21.494307\n",
      "97   0.000000\n",
      "98   0.000000\n",
      "99   0.000000\n",
      "\n",
      "[100 rows x 1 columns]\n",
      "51.8117939690401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    p = parameters\n",
    "    domain = ForestDomain()\n",
    "    paths_taken = [[] for _ in range(p[\"n_experiments\"])]\n",
    "    max_global_reward = []\n",
    "    for cf_id in range(p[\"n_experiments\"]):\n",
    "        domain.create_forest(cf_id)\n",
    "        \n",
    "    for cf_id in range(p[\"n_experiments\"]):\n",
    "        domain.setup_forest()\n",
    "        # Logging Params\n",
    "        max_global_rewards_over_experiment = []\n",
    "        \n",
    "        for episode in tqdm(range(p[\"n_episodes\"]), desc=\"Current Episode\"):\n",
    "            domain.reset(cf_id)\n",
    "            epsilon = p[\"epsilon\"]\n",
    "            decay = p[\"epsilon_decay_factor\"]\n",
    "    \n",
    "            # Logging Params\n",
    "            global_rewards_over_episode = []\n",
    "            \n",
    "            for step in range(p[\"n_epochs\"]):\n",
    "                domain.execute(epsilon, step, reward_type=p[\"reward_type\"])\n",
    "                global_rewards_over_episode.append(np.sum(domain.global_rewards))\n",
    "                epsilon *= decay\n",
    "                \n",
    "            max_global_reward_this_episode = max(global_rewards_over_episode)\n",
    "            max_global_rewards_over_experiment.append(max_global_reward_this_episode)\n",
    "    \n",
    "            if episode == p[\"n_episodes\"] - 1 :\n",
    "                for agent in domain.agents:\n",
    "                    paths_taken[cf_id].append(agent.path.copy())\n",
    "                    # print([(key,\": \",value) for key, value in agent.Qtable.items() if value != 0.0], \"\\n\\n\")\n",
    "                    \n",
    "            # print(f\"Episode {episode + 1}/{p[\"n_episodes\"]} completed.\")\n",
    "            \n",
    "        if cf_id == p[\"n_experiments\"] - 1:\n",
    "            max_global_reward = max_global_rewards_over_experiment.copy()\n",
    "        print(f\"Experiment {cf_id + 1}/{p[\"n_experiments\"]} completed.\")\n",
    "    \n",
    "    frame = pd.DataFrame(max_global_reward)\n",
    "    print(frame)\n",
    "    print(np.max(max_global_reward))\n",
    "    # print(\"\\n\", paths_taken)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
