{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f2a60bc-2920-4383-8d4c-106264cb3d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# All imports go here\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pygame\n",
    "import time\n",
    "import pickle\n",
    "import turtle\n",
    "import matplotlib.pyplot as plt\n",
    "# from parameters import parameters as p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38050443-a112-46a9-9564-4a72f7104716",
   "metadata": {},
   "source": [
    "## Parameters and Hyper Parameters\n",
    "\n",
    "<details>\n",
    "    <summary> Simulation Hyperparameters </summary>\n",
    "\n",
    "## Simulation Hyperparameters:\n",
    "1) Number_of_experiment : int\n",
    "2) Number_of_episodes : int\n",
    "3) Number_of_epochs : int\n",
    "4) POI_distribution : string\n",
    "5) Agent_distribution : string\n",
    "6) Reward_type : int\n",
    "</details>\n",
    "<details>\n",
    "    <summary> Domain Parameters </summary>\n",
    "\n",
    "## Domain Parameters:\n",
    "1) X_dimension : float\n",
    "2) y_dimension : float\n",
    "3) Number_of_POIs : int\n",
    "4) Number_of_agents : int\n",
    "</details>\n",
    "<details>\n",
    "    <summary> Fire Parameters </summary>\n",
    "\n",
    "## Fire Parameters:\n",
    "1) Value : float\n",
    "2) Level : int\n",
    "3) Coupling : int\n",
    "</details>\n",
    "<details>\n",
    "    <summary> Agent Parameters </summary>\n",
    "\n",
    "## Agent Parameters:\n",
    "1) Max_step : int\n",
    "2) Sensor_resolution : float\n",
    "3) Number_of_sectors : int\n",
    "4) Sensor_radius : float\n",
    "</details>\n",
    "<details>\n",
    "    <summary> Q-Learning Parameters </summary>\n",
    "\n",
    "## Q-Learning Parameters:\n",
    "1) Epsilon : float\n",
    "2) Epsilon_decay_factor : float\n",
    "3) Learning_rate : float\n",
    "4) Discount_factor : float\n",
    "</details>\n",
    "\n",
    "___\n",
    "<details>\n",
    "    <summary> Functions </summary>\n",
    "\n",
    "## Functions:\n",
    "1) \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c794de7b-dfa3-4645-b6c8-054f2ba90e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "# Domain Params\n",
    "parameters[\"x_dim\"] = 50.0\n",
    "parameters[\"y_dim\"] = 50.0\n",
    "parameters[\"n_pois\"] = 5\n",
    "parameters[\"n_agents\"] = 3\n",
    "\n",
    "# Fire Params\n",
    "parameters[\"value\"] = 100\n",
    "parameters[\"level\"] = int(2)\n",
    "parameters[\"hazard_coupling\"] =   {1: {\"coupling\": 1, \"opti_coupling\": 1},\n",
    "                                   2: {\"coupling\": 2, \"opti_coupling\": 2},\n",
    "                                   3: {\"coupling\": 3, \"opti_coupling\": 4},\n",
    "                                   4: {\"coupling\": 3, \"opti_coupling\": 5},\n",
    "                                  }\n",
    "parameters[\"coupling\"] = int(parameters[\"hazard_coupling\"].get(parameters[\"level\"], 1)[\"coupling\"])\n",
    "parameters[\"opti_coupling\"] = int(parameters[\"hazard_coupling\"].get(parameters[\"level\"], 1)[\"opti_coupling\"])\n",
    "\n",
    "# Agent Params\n",
    "parameters[\"max_step\"] = 1.5\n",
    "parameters[\"sensor_res\"] = 90.0 # Sector size in degrees\n",
    "parameters[\"n_sectors\"] = int(360.0 / parameters[\"sensor_res\"])\n",
    "parameters[\"sensor_radius\"] = 3.0\n",
    "parameters[\"observation_size\"] = int(2 * parameters[\"n_sectors\"])\n",
    "\n",
    "# Q-Learning Params\n",
    "parameters[\"epsilon\"] = 0.8\n",
    "parameters[\"epsilon_decay_factor\"] = 0.999\n",
    "parameters[\"learning_rate\"] = 0.1\n",
    "parameters[\"discount_factor\"] = 0.95\n",
    "\n",
    "# Simulation Hyperparams\n",
    "parameters[\"n_experiments\"] = 1\n",
    "parameters[\"n_episodes\"] = int(1000)\n",
    "parameters[\"n_epochs\"] = int(2000)\n",
    "parameters[\"poi_distribution\"] = \"Random\"\n",
    "parameters[\"agent_distribution\"] = \"OneRandom\" # OneRandom, AllRandom\n",
    "parameters[\"reward_type\"] = 2 # 0: Global rewards, 1: Difference Rewards, 2: D++ Rewards\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8b5f84-90e5-42fa-a053-f8844fc05495",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "<details>\n",
    "    <summary>Functions</summary>\n",
    "\n",
    "## Functions:\n",
    "1) save_poi_config_csv()\n",
    "2) save_agent_config_csv()\n",
    "3) get_angle()\n",
    "4) get_euclidean_distance()\n",
    "5) get_squared_distance()\n",
    "</details>\n",
    "\n",
    "## Description:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "442e5aa9-6c11-45ac-a7d6-798def627cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def save_poi_config_csv(pois_info, config_id):\n",
    "    \"\"\"\n",
    "    Saves POIs' configuration to a csv file in a folder called World_Config\n",
    "    \"\"\"\n",
    "    dir_name = './Configs'  # Intended directory for output files\n",
    "\n",
    "    if not os.path.exists(dir_name):  # If Data directory does not exist, create it\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "    pfile_name = os.path.join(dir_name, f'POI_Config{config_id}.csv')\n",
    "\n",
    "    with open(pfile_name, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for poi_id in range(len(pois_info)):\n",
    "            writer.writerow(pois_info[poi_id, :])\n",
    "\n",
    "    csvfile.close()\n",
    "\n",
    "def save_agent_config_csv(agent_infos, config_id):\n",
    "    \"\"\"\n",
    "    Saves Agents' configuration to a csv file in a folder called World_Config\n",
    "    \"\"\"\n",
    "    dir_name = './Configs'  # Intended directory for output files\n",
    "\n",
    "    if not os.path.exists(dir_name):  # If Data directory does not exist, create it\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "    pfile_name = os.path.join(dir_name, f'Agent_Config{config_id}.csv')\n",
    "\n",
    "    row = np.zeros(3)\n",
    "    with open(pfile_name, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for agent_id in range(len(agent_infos)):\n",
    "            writer.writerow(agent_infos[agent_id, :])\n",
    "\n",
    "    csvfile.close()\n",
    "\n",
    "def create_pickle_file(input_data, dir_name, file_name):\n",
    "    \"\"\"\n",
    "    Create a pickle file using provided data in the specified directory\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dir_name):  # If Data directory does not exist, create it\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "    path_name = os.path.join(dir_name, file_name)\n",
    "    rover_file = open(path_name, 'wb')\n",
    "    pickle.dump(input_data, rover_file)\n",
    "    rover_file.close()\n",
    "\n",
    "def get_angle(source_x, source_y, target_x, target_y, radians=False):\n",
    "    radian_angle = math.atan2(target_y - source_y, target_x - source_x)\n",
    "    if radians:\n",
    "        return radian_angle\n",
    "    return math.degrees(radian_angle)\n",
    "\n",
    "def get_euclidean_distance(source_x, source_y, target_x, target_y):\n",
    "    d = math.sqrt((target_y - source_y)**2 + (target_x - source_x)**2)\n",
    "    if d < 0.01:\n",
    "        d = 0.01 \n",
    "    return d\n",
    "\n",
    "def get_squared_distance(source_x, source_y, target_x, target_y):\n",
    "    d =(target_y - source_y)**2 + (target_x - source_x)**2\n",
    "    if d < 0.0001:\n",
    "        d = 0.0001 \n",
    "    return d\n",
    "\n",
    "def plot_cumulative_rewards_experiment(config_id, cumsum_values):\n",
    "    plt.figure(figsize=(12, 18))\n",
    "    values = cumsum_values[config_id]\n",
    "    # Plot the graph\n",
    "    plt.plot(values, marker='o', linestyle='-', color='b', label='Global Rewards')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Cumulative System Rewards')\n",
    "    plt.title('Cumulative Rewards vs Episodes')\n",
    "    \n",
    "    # Add grid and legend\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20956a91-4fe7-4ebd-8c43-51769bed2310",
   "metadata": {},
   "source": [
    "## Agent Class\n",
    "\n",
    "<details>\n",
    "    <summary> Parameters </summary>\n",
    "\n",
    "### Parameters:\n",
    "1) ID : int\n",
    "2) Location : (x: float, y: float, theta: float)\n",
    "4) Step_size : float\n",
    "5) Sensor_radius : float\n",
    "6) Sensor_resolution : float in degrees.\n",
    "7) Number_of_sectors : int (360/resolution)\n",
    "8) Current_observation : ndarray\n",
    "9) Past_observation : ndarray\n",
    "10) Action_space (up, down, left, right)\n",
    "11) Q-table : dict\n",
    "12) Local_reward : float\n",
    "13) Current_action_dir : float (angle phi)\n",
    "14) Learning_rate : float\n",
    "15) Discount_factor : float\n",
    "16) Epsilon : float \n",
    "17) Snapshot : list (stores parameters that remain same across configs)\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary> Functions </summary>\n",
    "    \n",
    "### Functions:\n",
    "1) reset()\n",
    "2) setup_sector_angles()\n",
    "3) get_sector_from_direction()\n",
    "4) get_direction_from_sector()\n",
    "5) move()\n",
    "6) turn()\n",
    "7) new_location()\n",
    "8) update_history()\n",
    "9) get_Qvalue()\n",
    "10) set_Qvalue()\n",
    "11) update_Qvalue()\n",
    "12) scan_surrounding()\n",
    "13) scan_for_pois()\n",
    "14) scan_for_agents()\n",
    "15) **select_action()**\n",
    "16) e_greedy()\n",
    "</details>\n",
    "\n",
    "### Desciption:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e8cad18-0ba0-4fa9-9f39-24fab9d786cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, agent_id, x_pos, y_pos, theta):\n",
    "        self.state_size = 8 # (size of the encoding for observations from environment) This needs to change when the actual observation size is known.\n",
    "        self.agent_id = agent_id\n",
    "        self.loc = np.array([x_pos, y_pos, theta])\n",
    "        self.max_step = p[\"max_step\"]\n",
    "        \n",
    "        # Sensor specs.\n",
    "        self.sensor_range = p[\"sensor_radius\"]\n",
    "        self.sensor_res = p[\"sensor_res\"]\n",
    "        self.n_sectors = p[\"n_sectors\"]\n",
    "        self.sector_angles = self.setup_sector_angles()\n",
    "        \n",
    "        # MDP params\n",
    "        self.curr_obs = np.zeros(self.state_size)\n",
    "        self.prev_obs = np.zeros(self.state_size)\n",
    "        self.action_space = [i for i in range(self.n_sectors)] # 0 - Front, 1 - Left, 2 - Back, 3 - Right\n",
    "        self.Qtable = {}\n",
    "        self.navigation = {}\n",
    "        self.l_reward = 0.0\n",
    "        self.curr_action_dir = 0.0\n",
    "        self.lr = p[\"learning_rate\"]\n",
    "        self.dscf = p[\"discount_factor\"]\n",
    "\n",
    "        # Logging info\n",
    "        self.snap = [self.lr, self.dscf]\n",
    "        self.path = [self.loc.copy()]\n",
    "\n",
    "    def reset(self, config):\n",
    "        self.loc[0] = config[0]\n",
    "        self.loc[1] = config[1]\n",
    "        self.loc[2] = config[2]\n",
    "        self.curr_obs = np.zeros(self.state_size)\n",
    "        self.prev_obs = self.curr_obs.copy()\n",
    "        self.l_reward = 0.0\n",
    "        self.curr_action_dir = 0.0\n",
    "        self.lr = self.snap[0]\n",
    "        self.dscf = self.snap[1]\n",
    "        self.path = [self.loc.copy()]\n",
    "\n",
    "    def setup_sector_angles(self):\n",
    "        half = self.sensor_res/2\n",
    "        angles = []\n",
    "        n = self.n_sectors\n",
    "        \n",
    "        s = 90.0 - half\n",
    "        for i in range(n):\n",
    "            angles.append(s)\n",
    "            s += self.sensor_res\n",
    "        return angles\n",
    "        \n",
    "    def get_sector_from_direction(self, direction):\n",
    "        # direction should be in range [0, 360)\n",
    "        # direction = direction - self.loc[2]  # This line will change direction from world coordinate system to local coordiate system.\n",
    "        if direction < 0:\n",
    "            while direction < 0:\n",
    "                direction += 360\n",
    "        elif direction >= 360:\n",
    "            while direction >= 360:\n",
    "                direction -= 360\n",
    "\n",
    "        for i in range(len(self.sector_angles) - 1):\n",
    "            if direction >= self.sector_angles[i] and direction < self.sector_angles[i + 1]:\n",
    "                # print(\"Quadrant = \", i)\n",
    "                return i, direction\n",
    "                \n",
    "        # print(\"Quadrant = \", len(self.sector_angles) - 1)\n",
    "        return len(self.sector_angles) - 1, direction\n",
    "\n",
    "    def get_direction_from_sector(self, sector):\n",
    "        direction = self.sector_angles[sector] + self.sensor_res/2\n",
    "        if direction >= 360.0:\n",
    "            direction -= 360.0\n",
    "        rad_angle = math.radians(direction) \n",
    "        dir_vector = [math.cos(rad_angle), math.sin(rad_angle)]\n",
    "        return direction, dir_vector\n",
    "\n",
    "    def move(self, new_location):\n",
    "        self.loc[:2] = new_location\n",
    "        self.update_history()\n",
    "\n",
    "    def turn(self, new_direction):\n",
    "        self.loc[2] = new_direction\n",
    "        self.update_history()\n",
    "\n",
    "    def new_location(self, new_location):\n",
    "        self.loc = new_location\n",
    "        self.update_history()\n",
    "    \n",
    "    def update_history(self):\n",
    "        self.path.append(self.loc.copy())\n",
    "\n",
    "    def get_nav_value(self, state, sector):\n",
    "        # For given position and action_sector provide a Q-value\n",
    "        return self.navigation.get(tuple(state))\n",
    "    \n",
    "    def get_Qvalue(self, observation, action):\n",
    "        return self.Qtable.get((tuple(observation), action), 0.0)\n",
    "\n",
    "    def set_Qvalue(self, observation, action, value):\n",
    "        self.Qtable[(tuple(observation), action)] = value\n",
    "    \n",
    "    def update_Qvalue(self):\n",
    "        curr_Q = self.get_Qvalue(self.curr_obs.tolist(), self.get_sector_from_direction(self.curr_action_dir))\n",
    "        next_Qs = [self.get_Qvalue(self.curr_obs.tolist(), next_sector) for next_sector in range(self.n_sectors)]\n",
    "        best_Q = np.max(next_Qs)\n",
    "        td_error = self.l_reward + self.dscf * (best_Q - curr_Q)\n",
    "        new_Q = curr_Q + self.lr * td_error\n",
    "        # Debug Statement:\n",
    "        if self.agent_id == 0:\n",
    "            if new_Q > 0.0:\n",
    "                print(self.prev_obs.tolist(), \":\", new_Q, \"\\n\\n\")\n",
    "        self.set_Qvalue(self.prev_obs.tolist(), self.get_sector_from_direction(self.curr_action_dir), new_Q)\n",
    "        self.l_reward = 0.0\n",
    "\n",
    "    def scan_surrounding(self, agents, pois, distance_table):\n",
    "        observation_for_pois = self.scan_for_pois(pois, distance_table)\n",
    "        observation_for_other_agents = self.scan_for_agents(agents)\n",
    "        self.prev_obs = self.curr_obs.copy()\n",
    "        self.curr_obs = np.concatenate((observation_for_pois, observation_for_other_agents))\n",
    "    \n",
    "    def scan_for_pois(self, pois, dist_table):\n",
    "        poi_state = np.zeros(self.n_sectors)\n",
    "        state_mask = np.zeros(self.n_sectors)\n",
    "        dist_sq_to_pois = dist_table[:, self.agent_id]\n",
    "        \n",
    "        detection_radius_sq = self.sensor_range**2\n",
    "        poi_ids_in_range = np.where(dist_sq_to_pois <= detection_radius_sq)[0]\n",
    "        # poi_ids_out_range = np.where(dist_sq_to_pois > detection_radius_sq)[0]\n",
    "        \n",
    "        for poi_id in poi_ids_in_range:\n",
    "            poi = pois[poi_id]\n",
    "            angle = get_angle(self.loc[0], self.loc[1], poi.loc[0], poi.loc[1])\n",
    "            poi_sector, angle = self.get_sector_from_direction(angle)\n",
    "            dist_sq = dist_sq_to_pois[poi_id]\n",
    "            if poi.poi_id in poi_ids_in_range:\n",
    "                # Detected the POI\n",
    "                state_mask[poi_sector] = 1\n",
    "                poi_state[poi_sector] += round(poi.val/dist_sq, 3) \n",
    "\n",
    "        for idx in range(self.n_sectors):\n",
    "            if state_mask[idx] == 0:\n",
    "                poi_state[idx] = -1\n",
    "        return poi_state\n",
    "    \n",
    "    def scan_for_agents(self, agents):\n",
    "        agent_state = np.zeros(self.n_sectors)\n",
    "        state_mask = np.zeros(self.n_sectors)\n",
    "\n",
    "        x_self, y_self, theta_self = self.loc\n",
    "        detection_radius_sq = self.sensor_range**2\n",
    "\n",
    "        for other_agent in agents:\n",
    "            if other_agent.agent_id == self.agent_id:\n",
    "                continue\n",
    "\n",
    "            x_other, y_other, theta_other = other_agent.loc\n",
    "            dist_sq = (x_other - x_self)**2 + (y_other - y_self)**2\n",
    "            if dist_sq < 0.01:\n",
    "                dist_sq = 0.01\n",
    "            if dist_sq <= detection_radius_sq:\n",
    "                angle = get_angle(x_self, y_self, x_other, y_other)\n",
    "                agent_sector, angle = self.get_sector_from_direction(angle)\n",
    "\n",
    "                state_mask[agent_sector] = 1\n",
    "                agent_state[agent_sector] += round(1/dist_sq, 3)\n",
    "        for idx in range(self.n_sectors):\n",
    "            if state_mask[idx] == 0:\n",
    "                agent_state[idx] = -1\n",
    "        return agent_state\n",
    "\n",
    "    def e_greedy(self, epsilon):\n",
    "        if np.random.rand() < epsilon:\n",
    "            # We explore\n",
    "            angle = np.random.uniform(0.0, 360.0)\n",
    "            rad_angle = math.radians(angle)\n",
    "            dir_vector = [math.cos(rad_angle), math.sin(rad_angle)]\n",
    "            return dir_vector, angle\n",
    "        else:\n",
    "            # We exploit\n",
    "            # q_values = []\n",
    "            max_Qvalue = -1000.0\n",
    "            best_action_sector = None\n",
    "            for action in range(self.n_sectors): # Action here is a sector of choice NOT a direction vector.\n",
    "                # q_values.append(self.get_Qvalue(self.curr_obs.tolist(), action))\n",
    "                Q_val = self.get_Qvalue(self.curr_obs.tolist(), action)\n",
    "                if Q_val > max_Qvalue:\n",
    "                    # found new max value\n",
    "                    best_action_sector = action\n",
    "                    max_Qvalue = Q_val\n",
    "            # max_Qvalue = np.max(q_values)\n",
    "            # best_action_sector = np.argmax(q_values)\n",
    "            \n",
    "            angle, dir_vector = self.get_direction_from_sector(best_action_sector)\n",
    "            return dir_vector, angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e87bf4-e8cf-41ba-9166-1b595464d624",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def get_sector_from_direction(res, n_sect, direction):\n",
    "#     # direction should be in range [0, 360)\n",
    "#     if direction < 0:\n",
    "#         while direction < 0:\n",
    "#             direction += 360\n",
    "#     elif direction >= 360:\n",
    "#         while direction >= 360:\n",
    "#             direction -= 360\n",
    "\n",
    "#     half = res/2\n",
    "#     angles = []\n",
    "#     n = n_sect\n",
    "        \n",
    "#     s = 90.0 - half\n",
    "    \n",
    "#     for i in range(n):\n",
    "#         angles.append(s)\n",
    "#         s += res\n",
    "        \n",
    "#     flag = False\n",
    "#     for i in range(len(angles) - 1):\n",
    "#         if direction >= angles[i] and direction < angles[i + 1]:\n",
    "#             flag = True\n",
    "#             print(\"Quadrant = \", i, direction)\n",
    "#     if not flag:        \n",
    "#         if (direction >= 0.0 and direction < angles[0]) or (direction >= angles[-1] and direction < 360): \n",
    "#             print(\"Quadrant = \", len(angles) - 1, direction)\n",
    "#     print(angles)\n",
    "#     return angles\n",
    "\n",
    "\n",
    "# def get_direction_from_sector(angles, res, sector):\n",
    "#     sector_angles = angles\n",
    "#     direction = sector_angles[sector] + res/2\n",
    "    \n",
    "#     # if sector >= len(sector_angles) - 1:\n",
    "#     if direction >= 360.0:\n",
    "#         direction -= 360.0\n",
    "#     print(direction)\n",
    "    \n",
    "# res = 60\n",
    "# n_sect = int(360/res)\n",
    "# sect_angles = get_sector_from_direction(res, n_sect, 40)\n",
    "# for i in range(n_sect):\n",
    "#     get_direction_from_sector(sect_angles, res, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31498530-1492-4638-bf1d-7970cafba20f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Point of Interest Class\n",
    "\n",
    "<details>\n",
    "    <summary> Parameters </summary>\n",
    "\n",
    "### Parameters:\n",
    "1) ID : int\n",
    "2) Location : (x: float, y: float)\n",
    "3) Value : float\n",
    "4) Level : int (what is the hazard level of POI)\n",
    "5) Coupling : int (minimum number of agents reqired)\n",
    "6) Detected : int (list of agent_ids that detected the poi)\n",
    "7) Done : bool (True if it's harvested, False otherwise)\n",
    "8) **Snapshot : list (stores parameters that stay constant across configs)**\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary> Functions </summary>\n",
    "    \n",
    "### Functions:\n",
    "1) reset()\n",
    "2) set_detected()\n",
    "3) set_done()\n",
    "</details>\n",
    "\n",
    "### Desciption:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a5e4f61-824e-49ae-bf24-e8fb3dd1bcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POI:\n",
    "    def __init__(self, poi_id, x_pos, y_pos, value, level, coupling, opti_coupling):\n",
    "        self.poi_id = int(poi_id)\n",
    "        self.loc = np.array([x_pos, y_pos])\n",
    "        self.val = value\n",
    "        self.level = int(level)\n",
    "        self.coupling = int(coupling)\n",
    "        self.opti_coupling = int(opti_coupling)\n",
    "        self.detected = []\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self, config):\n",
    "        self.loc[0] = config[0]\n",
    "        self.loc[1] = config[1]\n",
    "        self.val = config[2]\n",
    "        self.level = config[3]\n",
    "        self.coupling = config[4]\n",
    "        self.opti_coupling = config[5]\n",
    "        self.detected = []\n",
    "        self.done = False\n",
    "\n",
    "    def set_done(self, flag):\n",
    "        self.done = flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa1266f-aa90-42ef-9ce2-f4ec87cdef6f",
   "metadata": {},
   "source": [
    "## Forest Domain Class\n",
    "\n",
    "<details>\n",
    "    <summary> Parameters </summary>\n",
    "\n",
    "### Parameters:\n",
    "1) X : float\n",
    "2) Y : float\n",
    "3) Number of POI : int\n",
    "4) Number of Agents : int\n",
    "5) Done : bool (True if all fires are done, False otherwise)\n",
    "6) Global_rewards : list (one reward for each poi)\n",
    "7) Agents : list (objects of Agent Class)\n",
    "8) Agent_configurations : 2D List (stores different positional configurations of Agents) \n",
    "9) POIs : list (objects of POI Class)\n",
    "10) POI_configurations : 2D List (stores different positional configurations of POIs)\n",
    "11) POI_agent_distances : 2D array of floats (distance squared values for each poi and agent pair)\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary> Functions </summary>\n",
    "    \n",
    "### Functions:\n",
    "1) reset()\n",
    "2) **show_simulation_details()**\n",
    "3) goals_done()\n",
    "4) create_forest()\n",
    "5) setup_forest()\n",
    "6) create_poi_config()\n",
    "7) create_agent_config()\n",
    "6) load_poi_config()\n",
    "7) load_agent_config()\n",
    "8) update_distance_table()\n",
    "9) select_joint_action()\n",
    "10) calculate_global_reward()\n",
    "11) **calculate_difference_reward()**\n",
    "12) step()\n",
    "13) execute()\n",
    "</details>\n",
    "\n",
    "### Desciption:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87a21830-d895-488c-831f-c3bee6cdca05",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ForestDomain:\n",
    "    def __init__(self):\n",
    "        self.X = p[\"x_dim\"]\n",
    "        self.Y = p[\"y_dim\"]\n",
    "        self.n_pois = p[\"n_pois\"]\n",
    "        self.n_agents = p[\"n_agents\"]\n",
    "        self.done = False\n",
    "        self.global_rewards = np.zeros(self.n_pois)\n",
    "        self.agents = np.empty(self.n_agents, dtype=object)\n",
    "        self.agent_configs = [[] for _ in range(self.n_agents)]\n",
    "        self.pois = np.empty(self.n_pois, dtype=object)\n",
    "        self.poi_configs = [[] for _ in range(self.n_pois)]\n",
    "        self.distance_table = np.ones((self.n_pois, self.n_agents)) # stores squared eucliedian distances\n",
    "        \n",
    "    def reset(self, config_id):\n",
    "        self.done = False\n",
    "        self.global_rewards = np.zeros(self.n_pois)\n",
    "        self.distance_table[:, :] = 1000.0\n",
    "        for idx, poi in enumerate(self.pois):\n",
    "            poi.reset(self.poi_configs[idx][config_id])\n",
    "        for idx, agent in enumerate(self.agents):\n",
    "            agent.reset(self.agent_configs[idx][config_id])\n",
    "        \n",
    "    def goals_done(self):\n",
    "        for poi in self.pois:\n",
    "            if poi.done:\n",
    "                continue\n",
    "            else:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def create_forest(self, config_id):\n",
    "        poi_infos = self.create_poi_config()\n",
    "        save_poi_config_csv(poi_infos, config_id)\n",
    "        \n",
    "        agent_infos = self.create_agent_config(poi_infos)\n",
    "        save_agent_config_csv(agent_infos, config_id)\n",
    "        \n",
    "    def setup_forest(self):\n",
    "        f1 = self.load_poi_config()\n",
    "        # print(self.pois[:].poi_id)\n",
    "        f2 = self.load_agent_config()\n",
    "        if f1 and f2:\n",
    "            print(\"Forest Ready.\")\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def create_poi_config(self):\n",
    "        all_poi_params = np.zeros((self.n_pois, 7)) # (n_pois) X (poi_id, x_pos, y_pos, value, level, coupling, opti_coupling)\n",
    "        if p[\"poi_distribution\"] == \"Random\":\n",
    "            for idx in range(self.n_pois):\n",
    "                # all_poi_params[idx, 0] = int(idx)\n",
    "                \n",
    "                x = random.uniform(0, self.X - 1.0)\n",
    "                y = random.uniform(0, self.Y - 1.0)\n",
    "    \n",
    "                too_close = True\n",
    "                while too_close:\n",
    "                    count = 0\n",
    "                    for i in range(self.n_pois):\n",
    "                        if i != idx:\n",
    "                            x_dist = x - all_poi_params[idx, 1]\n",
    "                            y_dist = y - all_poi_params[idx, 2]\n",
    "                            dist_sq = x_dist** 2 + y_dist**2\n",
    "                            min_dist_sq = (p[\"sensor_radius\"] * 2.1)** 2\n",
    "                            if dist_sq <= min_dist_sq:\n",
    "                                count += 1\n",
    "                    if count == 0:\n",
    "                        too_close = False\n",
    "                    else:\n",
    "                        x = random.uniform(0, self.X - 1.0)\n",
    "                        y = random.uniform(0, self.Y - 1.0)\n",
    "                all_poi_params[idx, 0] = x\n",
    "                all_poi_params[idx, 1] = y\n",
    "                all_poi_params[idx, 2] = p[\"value\"]\n",
    "                all_poi_params[idx, 3] = p[\"level\"]\n",
    "                all_poi_params[idx, 4] = p[\"coupling\"]\n",
    "                all_poi_params[idx, 5] = p[\"opti_coupling\"]\n",
    "        print(\"POI Configurations generated.\")\n",
    "        return all_poi_params\n",
    "\n",
    "    def create_agent_config(self, poi_info):\n",
    "        all_agent_params = np.zeros((self.n_agents, 4)) # (n_agents) X (agent_id, x_pos, y_pos, theta)\n",
    "        if p[\"agent_distribution\"] == \"OneRandom\":\n",
    "            x_pos = random.uniform(0.0, self.X - 1.0)\n",
    "            y_pos = random.uniform(0.0, self.Y - 1.0)\n",
    "            theta = random.uniform(0.0, 360.0)\n",
    "            buffer = 3.0\n",
    "            \n",
    "            too_close = True\n",
    "            while too_close:\n",
    "                count = 0\n",
    "                for poi_idx in range(self.n_pois):\n",
    "                    x_dist = x_pos - poi_info[poi_idx, 1]\n",
    "                    y_dist = y_pos - poi_info[poi_idx, 2]\n",
    "                    dist_sq = x_dist**2 + y_dist**2\n",
    "                    min_dist_sq = (p[\"sensor_radius\"] * 2.0 + buffer)** 2\n",
    "                    if dist_sq <= min_dist_sq:\n",
    "                        count += 1\n",
    "                \n",
    "                if count == 0:\n",
    "                    too_close = False\n",
    "                else:\n",
    "                    x_pos = random.uniform(0.0, self.X - 1.0)\n",
    "                    y_pos = random.uniform(0.0, self.Y - 1.0)\n",
    "            \n",
    "            for idx in range(self.n_agents):\n",
    "                # all_agent_params[idx, 0] = int(idx)\n",
    "                all_agent_params[idx, 0] = x_pos\n",
    "                all_agent_params[idx, 1] = y_pos\n",
    "                all_agent_params[idx, 2] = theta\n",
    "                \n",
    "        # elif p[\"agent_distribution\"] == \"AllRandom\":\n",
    "        print(\"Agent Configurations generated.\")\n",
    "        return all_agent_params\n",
    "    \n",
    "    def load_poi_config(self):\n",
    "        for cf_id in range(p[\"n_experiments\"]):\n",
    "            csv_input = []\n",
    "            status = False\n",
    "            with open(f'./Configs/POI_Config{cf_id}.csv', mode='r') as csvfile:\n",
    "                csv_reader = csv.reader(csvfile, delimiter=',')\n",
    "                for row in csv_reader:\n",
    "                    csv_input.append(row)\n",
    "    \n",
    "            for idx in range(self.n_pois):\n",
    "                poi_id = idx #int(float(csv_input[idx][0]))\n",
    "                x_pos = float(csv_input[idx][0])\n",
    "                y_pos = float(csv_input[idx][1])\n",
    "                value = float(csv_input[idx][2])\n",
    "                level = int(float(csv_input[idx][3]))\n",
    "                coupling = int(float(csv_input[idx][4]))\n",
    "                opti_coupling = int(float(csv_input[idx][5]))\n",
    "                \n",
    "                if cf_id == 0:\n",
    "                    self.pois[idx] = POI(poi_id, x_pos, y_pos, value, level, coupling, opti_coupling)\n",
    "                    \n",
    "                self.poi_configs[poi_id].append((x_pos, y_pos, value, level, coupling, opti_coupling))\n",
    "\n",
    "        status = True\n",
    "        return status\n",
    "\n",
    "    def load_agent_config(self):\n",
    "        status = False\n",
    "        for cf_id in range(p[\"n_experiments\"]):\n",
    "            csv_input = []\n",
    "            with open(f'./Configs/Agent_Config{cf_id}.csv', mode='r') as csvfile:\n",
    "                csv_reader = csv.reader(csvfile, delimiter=',')\n",
    "\n",
    "                for row in csv_reader:\n",
    "                    csv_input.append(row)\n",
    "\n",
    "            for idx in range(self.n_agents):\n",
    "                agent_id = idx #int(float(csv_input[idx][0]))\n",
    "                x_pos = float(csv_input[idx][0])\n",
    "                y_pos = float(csv_input[idx][1])\n",
    "                theta = float(csv_input[idx][2])\n",
    "\n",
    "                if cf_id == 0:\n",
    "                    self.agents[idx] = Agent(agent_id, x_pos, y_pos, theta)\n",
    "\n",
    "                self.agent_configs[idx].append((x_pos, y_pos, theta))\n",
    "                \n",
    "        status = True\n",
    "        return status\n",
    "\n",
    "    def update_distance_table(self):\n",
    "        for poi in self.pois:\n",
    "            # for each poi\n",
    "            poi_x, poi_y = poi.loc\n",
    "            for agent in self.agents:\n",
    "                a_x, a_y, a_t = agent.loc\n",
    "                dist_sq = (poi_x - a_x)**2 + (poi_y - a_y)**2\n",
    "                if dist_sq < 0.01:\n",
    "                    dist_sq = 0.01\n",
    "                self.distance_table[poi.poi_id, agent.agent_id] = dist_sq\n",
    "        '''\n",
    "        print(\"ForestDomain:: Distance Table: \\n\")\n",
    "        for r in self.distance_table:\n",
    "            for c in r:\n",
    "                print(c, \" \")\n",
    "            print(\"\\n\")\n",
    "        '''\n",
    "    \n",
    "    def select_joint_action(self, epsilon):\n",
    "        joint_action = np.zeros((self.n_agents, 2)) # returns x, y direction vector for action taken.\n",
    "        for agent in self.agents:\n",
    "            action, angle = agent.e_greedy(epsilon)\n",
    "            agent.curr_action_dir = angle\n",
    "            joint_action[agent.agent_id] = action\n",
    "        return joint_action\n",
    "    \n",
    "    def calc_global_reward(self):\n",
    "        global_reward = np.zeros(self.n_pois)\n",
    "        # Calculate the global rewards for each POI.\n",
    "        for poi in self.pois:\n",
    "            if poi.done:\n",
    "                continue\n",
    "            observers = 0\n",
    "            poi_reward = 0.0\n",
    "            # Check which agents have detected the POI.\n",
    "            detection_radius_sq = (p[\"sensor_radius\"]/2.0)**2\n",
    "            observer_agent_ids = np.where(self.distance_table[poi.poi_id] <= detection_radius_sq)[0]\n",
    "            observers = len(observer_agent_ids)\n",
    "            if observers > 0:\n",
    "                for agent in self.agents:\n",
    "                    # Provide 10% reward for detecting the poi and give rewards untill poi is not done.\n",
    "                    if agent.agent_id in observer_agent_ids:\n",
    "                        if agent.agent_id not in poi.detected:\n",
    "                            poi.detected.append(agent.agent_id)\n",
    "                        agent.l_reward = poi.val * 0.1\n",
    "                        \n",
    "            # Get just observer count\n",
    "            '''\n",
    "            sorted_dist_sq_from_poi = np.sort(self.distance_table[poi.poi_id])\n",
    "            for i in range(poi.coupling):\n",
    "                if sorted_dist_sq_from_poi[i] < detection_radius_sq:\n",
    "                    observers += 1\n",
    "             ''' \n",
    "            if observers >= int(poi.coupling):\n",
    "                # We have minimum required agents\n",
    "                dist_sq_sum = np.sum(np.partition(self.distance_table[poi.poi_id], observers-1)[:observers])\n",
    "                v = poi.val/dist_sq_sum\n",
    "                reward_full = v  # Add any other partial rewards gained in the process here.\n",
    "                optimum_coupling = poi.opti_coupling  # When there is a different optimum value than the minimum, change this. \n",
    "                global_reward[poi.poi_id] = reward_full * (np.e/float(optimum_coupling)) * np.exp(-1 * float(observers/optimum_coupling)) \n",
    "                poi_status = True\n",
    "                # If the agent needs to be on the POI's location to complete it. Un-comment the block below. \n",
    "                ''' \n",
    "                observers_on_poi = 0\n",
    "                for i in range(observers_in_range):\n",
    "                    if sorted_dist_sq_from_poi[i] <= 0.001:\n",
    "                        # Has reached poi\n",
    "                        continue\n",
    "                    else:\n",
    "                        # Atleast 1 is missing.\n",
    "                        flag = False\n",
    "                '''\n",
    "                poi.set_done(poi_status)\n",
    "        self.global_rewards = global_reward\n",
    "        \n",
    "    def calc_difference_reward(self):\n",
    "        difference_rewards = np.zeros(self.n_agents)\n",
    "        for agent in self.agents:\n",
    "            counterfactual = 0.0\n",
    "            counterfactual_rewards = np.zeros(self.n_pois)\n",
    "            detection_radius_sq = (agent.sensor_range/2.0)**2\n",
    "            for poi in self.pois:\n",
    "                observers = 0\n",
    "                distances = self.distance_table[poi.poi_id, :]\n",
    "                agents_in_radius = distances <= detection_radius_sq\n",
    "                agents_in_radius[agent.agent_id] = False\n",
    "\n",
    "                observers = np.sum(agents_in_radius)\n",
    "                if observers >= poi.coupling:\n",
    "                    dist_sq_sum = np.sum(distances[agents_in_radius])\n",
    "                    v = poi.val/dist_sq_sum\n",
    "                    counterfactual_rewards[poi.poi_id] = v * (np.e / float(poi.opti_coupling)) * np.exp(-1 * float(observers/poi.opti_coupling))\n",
    "            difference_rewards[agent.agent_id] = np.sum(self.global_rewards - counterfactual_rewards)\n",
    "        return difference_rewards  \n",
    "\n",
    "    def calc_difference_plus_plus(self):\n",
    "        difference_rewards = self.calc_difference_reward()\n",
    "        dpp_rewards = np.zeros(self.n_agents)\n",
    "        n_ghost_agents = self.n_agents - 1\n",
    "        n_ghost_opti = 0\n",
    "        for agent in self.agents:\n",
    "            detection_radius_sq = (agent.sensor_range/2.0)**2\n",
    "            ghost_global = np.zeros(self.n_pois)\n",
    "            \n",
    "            n_ghost = 1\n",
    "            while n_ghost <= n_ghost_agents:\n",
    "                poi_rewards = np.zeros(self.n_pois)\n",
    "                for poi in self.pois:\n",
    "                    observers = 0\n",
    "                    dist_sq = self.distance_table[poi.poi_id]\n",
    "                    ghost_dist_to_add = np.full(n_ghost, dist_sq[agent.agent_id])\n",
    "                    dist_sq_ghost = np.append(dist_sq, dist_sq[agent.agent_id])\n",
    "                    agents_in_radius = dist_sq_ghost <= detection_radius_sq\n",
    "                    observers = np.sum(agents_in_radius)\n",
    "    \n",
    "                    if observers >= poi.coupling:\n",
    "                        dist_sq_sum = np.sum(dist_sq_ghost[agents_in_radius])\n",
    "                        v = poi.val/dist_sq_sum\n",
    "                        poi_rewards[poi.poi_id] = v * (np.e / float(poi.opti_coupling)) * np.exp(-1 * float(observers/poi.opti_coupling))\n",
    "                if np.sum(poi_rewards) > np.sum(ghost_global):\n",
    "                    ghost_global = poi_rewards.copy()\n",
    "                    n_ghost_opti = n_ghost\n",
    "                    n_ghost += 1\n",
    "                else:\n",
    "                    # Found the optimal in previous iteration.\n",
    "                    break\n",
    "            # ghost_global has the global value with n_ghost agents added.\n",
    "            dpp_rewards[agent.agent_id] = np.sum(ghost_global - self.global_rewards)/(n_ghost_opti + 1.0)\n",
    "            \n",
    "        return np.maximum(dpp_rewards, difference_rewards)\n",
    "    \n",
    "    def step(self, joint_action):\n",
    "        # Take the joint_action\n",
    "        for agent in self.agents:\n",
    "            # Calculate displacement.\n",
    "            dx = 2 * agent.max_step * (joint_action[agent.agent_id, 0] - 0.5)\n",
    "            dy = 2 * agent.max_step * (joint_action[agent.agent_id, 1] - 0.5)\n",
    "\n",
    "            # Get new coordinates in world.\n",
    "            x = np.clip(agent.loc[0] + dx, 0, self.X - 1)\n",
    "            y = np.clip(agent.loc[1] + dy, 0, self.Y - 1)\n",
    "\n",
    "            # Move the agent to new position.\n",
    "            agent.move((x, y))\n",
    "\n",
    "        # Update Distance table.\n",
    "        self.update_distance_table()\n",
    "        \n",
    "        # Fetch new observations.\n",
    "        for agent in self.agents:\n",
    "            agent.scan_surrounding(self.agents, self.pois, self.distance_table)\n",
    "\n",
    "        # Calculate global reswards for the system after taking this step.\n",
    "        self.calc_global_reward()\n",
    "\n",
    "    def execute(self, epsilon, step,  reward_type=0):\n",
    "        joint_action = self.select_joint_action(epsilon)\n",
    "\n",
    "        self.step(joint_action)\n",
    "        rewards = np.zeros(self.n_agents)\n",
    "        match reward_type:\n",
    "            case 0:\n",
    "                # Global rewards\n",
    "                reward = np.full(self.n_agents, np.sum(self.global_rewards))\n",
    "            case 1:\n",
    "                # Difference rewards\n",
    "                reward = self.calc_difference_reward()\n",
    "                # print(\"Using Difference Rewards.\")\n",
    "            case 2:\n",
    "                # D++ rewards\n",
    "                reward = self.calc_difference_plus_plus()\n",
    "                # print(\"Using D++ Rewards.\")\n",
    "            case _:\n",
    "                print(\"Incorrect Reward code.\")\n",
    "                \n",
    "        for agent in self.agents:\n",
    "            agent.l_reward += reward[agent.agent_id]\n",
    "            agent.update_Qvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c250d5ab-83b7-4d85-b5b1-8a6db2db8f37",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def load_poi_config(n_pois, X, Y, radius, value, level, coupling):\n",
    "#     all_poi_params = np.zeros((n_pois, 6)) # (n_pois) X (poi_id, x_pos, y_pos, value, level, coupling)\n",
    "#     pois = np.empty(n_pois, dtype=object)\n",
    "#     for idx in range(n_pois):\n",
    "        # all_poi_params[idx, 0] = idx\n",
    "#         x = random.uniform(0, X - 1.0)\n",
    "#         y = random.uniform(0, Y - 1.0)\n",
    "\n",
    "#         too_close = True\n",
    "#         while too_close:\n",
    "#             count = 0\n",
    "#             for i in range(n_pois):\n",
    "#                 if i != idx:\n",
    "#                     x_dist = x - all_poi_params[idx, 1]\n",
    "#                     y_dist = y - all_poi_params[idx, 2]\n",
    "#                     dist_sq = x_dist** 2 + y_dist**2\n",
    "#                     min_dist_sq = (radius * 2.1)** 2\n",
    "#                     if dist_sq < min_dist_sq:\n",
    "#                         count += 1\n",
    "#             if count == 0:\n",
    "#                 too_close = False\n",
    "#             else:\n",
    "#                 x = random.uniform(0, X - 1.0)\n",
    "#                 y = random.uniform(0, Y - 1.0)\n",
    "#         all_poi_params[idx, 1] = x\n",
    "#         all_poi_params[idx, 2] = y\n",
    "#         all_poi_params[idx, 3] = value\n",
    "#         all_poi_params[idx, 4] = level\n",
    "#         all_poi_params[idx, 5] = coupling\n",
    "#         pois[idx] = POI(*all_poi_params[idx])\n",
    "#     for poi in pois:\n",
    "#         print(poi.poi_id, poi.loc, poi.val)\n",
    "\n",
    "# load_poi_config(2, 100.0, 100.0, 3, 100, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02582fd3-76b9-45d8-a1b7-aeded4fa3fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a7c61c5-a096-4e7d-a3f8-7130da2d5527",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__ == \"__main__\":\\n    p = parameters\\n    domain = ForestDomain()\\n    paths_taken = [[] for _ in range(p[\"n_experiments\"])]\\n    cumsum_global_reward = []\\n    # for cf_id in range(p[\"n_experiments\"]):\\n    #     domain.create_forest(cf_id)\\n        \\n    for cf_id in range(p[\"n_experiments\"]):\\n        domain.setup_forest()\\n        # Logging Params\\n        cumsum_global_rewards_over_experiment = []\\n        \\n        for episode in tqdm(range(p[\"n_episodes\"]), desc=\"Current Episode\"):\\n            # print(\"**********************\\n\", domain.agents[0].Qtable.values() if domain.agents[0].Qtable.values() != 0.0 else \"\", \"\\n\\n\\n\")\\n            domain.reset(cf_id)\\n            # print(domain.agents[0].Qtable.values(), \"***********************\\n\")\\n            epsilon = p[\"epsilon\"]\\n            decay = p[\"epsilon_decay_factor\"]\\n    \\n            # Logging Params\\n            global_rewards_over_episode = []\\n            \\n            for step in range(p[\"n_epochs\"]):\\n                domain.execute(epsilon, step, reward_type=p[\"reward_type\"])\\n                global_rewards_over_episode.append(np.sum(domain.global_rewards))\\n                epsilon *= decay\\n                \\n            cumsum_global_rewards_over_experiment.append(np.cumsum(global_rewards_over_episode)[-1])\\n    \\n            if episode == p[\"n_episodes\"] - 1 :\\n                for agent in domain.agents:\\n                    paths_taken[cf_id].append(agent.path.copy())\\n                    # print([(key,\": \",value) for key, value in agent.Qtable.items() if value != 0.0], \"\\n\\n\")\\n                    \\n            # print(f\"Episode {episode + 1}/{p[\"n_episodes\"]} completed.\")\\n            \\n        # if cf_id == p[\"n_experiments\"] - 1:\\n        cumsum_global_reward.append(cumsum_global_rewards_over_experiment.copy())\\n        print(f\"Experiment {cf_id + 1}/{p[\"n_experiments\"]} completed.\")\\n\\n        # Save agent_paths using pickle\\n        create_pickle_file(paths_taken[cf_id],\"Output_Data/\",(\"EXP{0}_Agent_path\").format(cf_id))\\n        create_pickle_file(cumsum_global_rewards_over_experiment, \"Output_Data/\", (\"EXP{0}_cumulative_G\").format(cf_id))\\n    \\n    frame = pd.DataFrame(cumsum_global_reward)\\n    print(frame)\\n    print(np.max(cumsum_global_reward))\\n    plot_cumulative_rewards_experiment(0, cumsum_global_reward)\\n    # print(\"\\n\", paths_taken)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "    p = parameters\n",
    "    domain = ForestDomain()\n",
    "    paths_taken = [[] for _ in range(p[\"n_experiments\"])]\n",
    "    cumsum_global_reward = []\n",
    "    # for cf_id in range(p[\"n_experiments\"]):\n",
    "    #     domain.create_forest(cf_id)\n",
    "        \n",
    "    for cf_id in range(p[\"n_experiments\"]):\n",
    "        domain.setup_forest()\n",
    "        # Logging Params\n",
    "        cumsum_global_rewards_over_experiment = []\n",
    "        \n",
    "        for episode in tqdm(range(p[\"n_episodes\"]), desc=\"Current Episode\"):\n",
    "            # print(\"**********************\\n\", domain.agents[0].Qtable.values() if domain.agents[0].Qtable.values() != 0.0 else \"\", \"\\n\\n\\n\")\n",
    "            domain.reset(cf_id)\n",
    "            # print(domain.agents[0].Qtable.values(), \"***********************\\n\")\n",
    "            epsilon = p[\"epsilon\"]\n",
    "            decay = p[\"epsilon_decay_factor\"]\n",
    "    \n",
    "            # Logging Params\n",
    "            global_rewards_over_episode = []\n",
    "            \n",
    "            for step in range(p[\"n_epochs\"]):\n",
    "                domain.execute(epsilon, step, reward_type=p[\"reward_type\"])\n",
    "                global_rewards_over_episode.append(np.sum(domain.global_rewards))\n",
    "                epsilon *= decay\n",
    "                \n",
    "            cumsum_global_rewards_over_experiment.append(np.cumsum(global_rewards_over_episode)[-1])\n",
    "    \n",
    "            if episode == p[\"n_episodes\"] - 1 :\n",
    "                for agent in domain.agents:\n",
    "                    paths_taken[cf_id].append(agent.path.copy())\n",
    "                    # print([(key,\": \",value) for key, value in agent.Qtable.items() if value != 0.0], \"\\n\\n\")\n",
    "                    \n",
    "            # print(f\"Episode {episode + 1}/{p[\"n_episodes\"]} completed.\")\n",
    "            \n",
    "        # if cf_id == p[\"n_experiments\"] - 1:\n",
    "        cumsum_global_reward.append(cumsum_global_rewards_over_experiment.copy())\n",
    "        print(f\"Experiment {cf_id + 1}/{p[\"n_experiments\"]} completed.\")\n",
    "\n",
    "        # Save agent_paths using pickle\n",
    "        create_pickle_file(paths_taken[cf_id],\"Output_Data/\",(\"EXP{0}_Agent_path\").format(cf_id))\n",
    "        create_pickle_file(cumsum_global_rewards_over_experiment, \"Output_Data/\", (\"EXP{0}_cumulative_G\").format(cf_id))\n",
    "    \n",
    "    frame = pd.DataFrame(cumsum_global_reward)\n",
    "    print(frame)\n",
    "    print(np.max(cumsum_global_reward))\n",
    "    plot_cumulative_rewards_experiment(0, cumsum_global_reward)\n",
    "    # print(\"\\n\", paths_taken)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1b7670e-c1cb-4fc3-8f4a-cd23f546b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_cumulative_rewards_experiment(0, cumsum_global_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8268b64a-b334-464c-a679-6035528b196c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Visualization \n",
    "\n",
    "We have used PyGame to visualize the simulation of agents in the world.\n",
    "\n",
    "<details>\n",
    "    <summary> Parameters </summary>\n",
    "\n",
    "### Parameters:\n",
    "1) \n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary> Functions </summary>\n",
    "\n",
    "### Functions:\n",
    "1)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce35e287-8bba-4f4b-a965-08e62bcabead",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "pygame.init()\n",
    "\n",
    "# Window dimensions\n",
    "WIDTH, HEIGHT = 800, 600\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"Rover Domain Simulation\")\n",
    "\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "RED = (255, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "BLUE = (0, 0, 255)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d2909e-fe47-4c18-8540-ee52bd8d00a2",
   "metadata": {},
   "source": [
    "import pygame\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Window dimensions\n",
    "WIDTH, HEIGHT = 800, 600\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"Rover Domain Simulation\")\n",
    "\n",
    "# Colors\n",
    "WHITE = (255, 255, 255)\n",
    "BLUE = (0, 0, 255)\n",
    "RED = (255, 0, 0)\n",
    "\n",
    "# Load a fire image for the POI\n",
    "fire_image = pygame.image.load(\"fire.png\")  # Replace with your fire image path\n",
    "fire_image = pygame.transform.scale(fire_image, (30, 30))  # Scale to desired size\n",
    "\n",
    "\n",
    "class POI:\n",
    "    def __init__(self, x, y, coupling_required):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.coupling_required = coupling_required  # Number of agents required\n",
    "        self.agents_at_poi = set()  # Track agents currently at the POI\n",
    "        self.active = True\n",
    "\n",
    "    def check_coupling(self):\n",
    "        \"\"\"Check if the coupling requirement is satisfied.\"\"\"\n",
    "        if len(self.agents_at_poi) >= self.coupling_required:\n",
    "            self.active = False\n",
    "\n",
    "    def draw(self, screen):\n",
    "        \"\"\"Draw the POI if active.\"\"\"\n",
    "        if self.active:\n",
    "            screen.blit(fire_image, (self.x, self.y))\n",
    "\n",
    "\n",
    "class Rover:\n",
    "    def __init__(self, x, y, id):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.id = id  # Unique identifier for the rover\n",
    "        self.color = BLUE\n",
    "        self.size = 20\n",
    "\n",
    "    def move(self, dx, dy):\n",
    "        self.x += dx\n",
    "        self.y += dy\n",
    "\n",
    "    def draw(self, screen):\n",
    "        pygame.draw.rect(screen, self.color, pygame.Rect(self.x, self.y, self.size, self.size))\n",
    "\n",
    "    def is_at_poi(self, poi):\n",
    "        \"\"\"Check if the rover is at the POI.\"\"\"\n",
    "        return (\n",
    "            poi.x <= self.x <= poi.x + 30 and\n",
    "            poi.y <= self.y <= poi.y + 30\n",
    "        )\n",
    "\n",
    "\n",
    "def run_simulation():\n",
    "    # Initialize rovers and POI\n",
    "    rovers = [Rover(100, 100, id=1), Rover(200, 200, id=2)]  # Multiple rovers\n",
    "    poi = POI(400, 300, coupling_required=2)  # POI requiring 2 agents\n",
    "\n",
    "    clock = pygame.time.Clock()\n",
    "\n",
    "    running = True\n",
    "    while running:\n",
    "        screen.fill(WHITE)  # Clear screen\n",
    "\n",
    "        # Handle events (e.g., closing the window)\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "\n",
    "        # Control rover movement (use arrow keys for the first rover, WASD for the second)\n",
    "        keys = pygame.key.get_pressed()\n",
    "        if keys[pygame.K_LEFT]:\n",
    "            rovers[0].move(-5, 0)\n",
    "        if keys[pygame.K_RIGHT]:\n",
    "            rovers[0].move(5, 0)\n",
    "        if keys[pygame.K_UP]:\n",
    "            rovers[0].move(0, -5)\n",
    "        if keys[pygame.K_DOWN]:\n",
    "            rovers[0].move(0, 5)\n",
    "\n",
    "        if keys[pygame.K_a]:\n",
    "            rovers[1].move(-5, 0)\n",
    "        if keys[pygame.K_d]:\n",
    "            rovers[1].move(5, 0)\n",
    "        if keys[pygame.K_w]:\n",
    "            rovers[1].move(0, -5)\n",
    "        if keys[pygame.K_s]:\n",
    "            rovers[1].move(0, 5)\n",
    "\n",
    "        # Check which rovers are at the POI\n",
    "        poi.agents_at_poi = {rover.id for rover in rovers if rover.is_at_poi(poi)}\n",
    "        poi.check_coupling()\n",
    "\n",
    "        # Draw the rovers and POI\n",
    "        for rover in rovers:\n",
    "            rover.draw(screen)\n",
    "\n",
    "        poi.draw(screen)\n",
    "\n",
    "        # Update the display\n",
    "        pygame.display.update()\n",
    "\n",
    "        # Limit frame rate\n",
    "        clock.tick(30)\n",
    "\n",
    "    pygame.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_simulation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e26ee473-3654-4ee1-8dbb-771fa806a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_agent_paths(config_id):\n",
    "    \"\"\"\n",
    "    Import rover paths from pickle file\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dir_name = 'Output_Data/'\n",
    "    file_name = f'EXP{config_id}_Agent_path'\n",
    "    rover_path_file = os.path.join(dir_name, file_name)\n",
    "    infile = open(rover_path_file, 'rb')\n",
    "    rover_paths = pickle.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "    return rover_paths\n",
    "\n",
    "def import_poi_information(n_poi, config_id):\n",
    "    \"\"\"\n",
    "    Import POI information from saved configuration files\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pois = np.zeros((n_poi, 6))\n",
    "\n",
    "    config_input = []\n",
    "    with open(f'./Configs/POI_Config{config_id}.csv') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile, delimiter=',')\n",
    "\n",
    "        for row in csv_reader:\n",
    "            config_input.append(row)\n",
    "\n",
    "    for poi_id in range(n_poi):\n",
    "        pois[poi_id, 0] = float(config_input[poi_id][0])\n",
    "        pois[poi_id, 1] = float(config_input[poi_id][1])\n",
    "        pois[poi_id, 2] = float(config_input[poi_id][2])\n",
    "        pois[poi_id, 3] = float(config_input[poi_id][3])\n",
    "        pois[poi_id, 4] = float(config_input[poi_id][4])\n",
    "        pois[poi_id, 5] = float(config_input[poi_id][5])\n",
    "    return pois\n",
    "\n",
    "def stop_viz(screen):\n",
    "    screen.bye()\n",
    "\n",
    "p = parameters\n",
    "\n",
    "def run_agent_visualizer(config_id):\n",
    "    # Define screen parameters for the\n",
    "    screen_width = p[\"x_dim\"]*10\n",
    "    screen_height = p[\"y_dim\"]*10\n",
    "    screen = turtle.Screen()\n",
    "    screen.setup(screen_width+20, screen_height+20)  # define pixel width and height of screen\n",
    "    screen.title(\"Rover Domain\")\n",
    "    screen.bgcolor(\"white\")\n",
    "    screen.tracer(0)\n",
    "    \n",
    "    rovers = []\n",
    "    rover_paths = import_agent_paths(config_id)\n",
    "    for rov_id in range(p[\"n_agents\"]):\n",
    "        rovers.append(turtle.Turtle())\n",
    "        rovers[rov_id].shape(\"circle\")\n",
    "        rovers[rov_id].shapesize(10 / 20)  # Number of pixels you want / 20 (default size)\n",
    "        rovers[rov_id].color(\"#{:02X}{:02X}{:02X}\".format(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)))\n",
    "\n",
    "    pois = []\n",
    "    poi_info = import_poi_information(p[\"n_pois\"], config_id)\n",
    "    for poi_id in range(p[\"n_pois\"]):\n",
    "        pois.append(turtle.Turtle())\n",
    "        pois[poi_id].shape(\"triangle\")\n",
    "        pois[poi_id].shapesize(20 / 20)  # Number of pixels you want / 20 (default size)\n",
    "        pois[poi_id].color(\"red\")\n",
    "        pois[poi_id].penup()\n",
    "        # Convert rover units to pixel units used by screen\n",
    "        px = ((poi_info[poi_id, 0]/p[\"x_dim\"]) * screen_width) - (screen_width/2)\n",
    "        py = ((poi_info[poi_id, 1]/p[\"y_dim\"]) * screen_height) - (screen_height/2)\n",
    "        pois[poi_id].goto(px, py)\n",
    "        pois[poi_id].stamp()\n",
    "\n",
    "    # for srun in range(p[\"n_episodes\"]):\n",
    "    for tstep in range(p[\"n_epochs\"]):\n",
    "        screen.title(f\"Rover Domain{tstep}\")\n",
    "        for rov_id in range(p[\"n_agents\"]):\n",
    "            rovers[rov_id].clearstamps()\n",
    "            rovx = ((rover_paths[rov_id][tstep][0]/p[\"x_dim\"])*screen_width) - (screen_width/2)\n",
    "            rovy = ((rover_paths[rov_id][tstep][1]/p[\"y_dim\"])*screen_height) - (screen_height/2)\n",
    "            rovers[rov_id].goto(rovx, rovy)\n",
    "            rovers[rov_id].stamp()\n",
    "        screen.update()\n",
    "        time.sleep(1.0/30)\n",
    "        \n",
    "    # screen.listen()  # Set the screen to listen for key presses\n",
    "    # screen.onkeypress(stop_viz(screen), \"q\")\n",
    "    turtle.done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6d5d118-6b35-4d2b-bea8-63ac47ee4da7",
   "metadata": {},
   "outputs": [
    {
     "ename": "Terminator",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTerminator\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_agent_visualizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 84\u001b[0m, in \u001b[0;36mrun_agent_visualizer\u001b[1;34m(config_id)\u001b[0m\n\u001b[0;32m     82\u001b[0m         rovers[rov_id]\u001b[38;5;241m.\u001b[39mgoto(rovx, rovy)\n\u001b[0;32m     83\u001b[0m         rovers[rov_id]\u001b[38;5;241m.\u001b[39mstamp()\n\u001b[1;32m---> 84\u001b[0m     \u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# screen.listen()  # Set the screen to listen for key presses\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# screen.onkeypress(stop_viz(screen), \"q\")\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\MAS_A\\Lib\\turtle.py:1295\u001b[0m, in \u001b[0;36mTurtleScreen.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tracing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mturtles():\n\u001b[1;32m-> 1295\u001b[0m     \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1296\u001b[0m     t\u001b[38;5;241m.\u001b[39m_drawturtle()\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tracing \u001b[38;5;241m=\u001b[39m tracing\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\MAS_A\\Lib\\turtle.py:2654\u001b[0m, in \u001b[0;36mRawTurtle._update_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 2654\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_incrementudc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39m_updatecounter \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2656\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\MAS_A\\Lib\\turtle.py:1284\u001b[0m, in \u001b[0;36mTurtleScreen._incrementudc\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TurtleScreen\u001b[38;5;241m.\u001b[39m_RUNNING:\n\u001b[0;32m   1283\u001b[0m     TurtleScreen\u001b[38;5;241m.\u001b[39m_RUNNING \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Terminator\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tracing \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_updatecounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTerminator\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_agent_visualizer(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bd79ce6-cd94-4983-9e3d-c4730aa38d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e1a910f-41ab-4e35-bfa6-612f8d5df7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val *= 0.999\n",
    "# val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
